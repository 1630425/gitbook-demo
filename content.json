{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"禾呈哥哥","url":"https://1630425.github.io"},"pages":[{"title":"禾呈哥哥","date":"2019-06-05T03:03:41.060Z","updated":"2019-06-05T03:03:41.060Z","comments":true,"path":"about/index.html","permalink":"https://1630425.github.io/about/index.html","excerpt":"","text":"我们之所以觉得悬崖上的花朵美丽那是因为我们会在悬崖停下脚步而不是像那些毫不畏惧的花朵般能向天空踏出一步 支付宝红包码"}],"posts":[{"title":"flume-ng-sql-source编译jar","slug":"flume-ng-sql-source编译jar","date":"2019-01-28T10:07:16.000Z","updated":"2019-06-05T03:03:41.008Z","comments":true,"path":"posts/30026bfd.html","link":"","permalink":"https://1630425.github.io/posts/30026bfd.html","excerpt":"","text":"编译项目地址：https://github.com/keedio/flume-ng-sql-sourceFlume-ng-sql-source 1.5.2下载地址：https://codeload.github.com/keedio/flume-ng-sql-source/tar.gz/v1.5.21234tar -xzvf flume-ng-sql-source-1.5.2.tar.gzcd flume-ng-sql-source-1.5.2/mvn install -DskipTests -Dtarcd target 将flume-ng-sql-source-1.5.2.jar复制到FLUME_HOME/lib。编译后的flume-ng-sql-source-1.5.2.jar，下载地址：http://wangpant.cn/resource/view/hxgyv4nhjwqs.html","categories":[{"name":"数据采集","slug":"数据采集","permalink":"https://1630425.github.io/categories/数据采集/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://1630425.github.io/tags/Flume/"},{"name":"flume-ng-sql-source","slug":"flume-ng-sql-source","permalink":"https://1630425.github.io/tags/flume-ng-sql-source/"}]},{"title":"Flume修改SQLSource以针对时间戳增量数据传输","slug":"flume修改sqlsource以针对时间戳增量数据传输","date":"2019-01-14T06:19:52.000Z","updated":"2019-06-05T03:03:41.008Z","comments":true,"path":"posts/3f943cf8.html","link":"","permalink":"https://1630425.github.io/posts/3f943cf8.html","excerpt":"","text":"flume github关于增量数据传输的原理，是通过唯一id，递增，每次记录传输的数据量+current_index=last_index，只能识别新增数据，检测不到删除与更新。不符合没有增量id的情况。由于数据存在时间戳标志，因此改写flume sqlsource以应对实际需求： 1. 每次增量传输先查询数据库中当前最大的时间戳，记录为maxtime2. 查询数据库：select * from table where time&gt;=current_index and time&lt;maxtime,此时不能取到time=maxtime的数据，不排除在数据查询之后会继续生成maxtime的新数据，则会出现数据遗漏3. 增量数据操作完成，将current_index=maxtime，写入状态表 SQLSourceHelper增加以下两段代码：1234567891011121314151617181920//增加取数据库最大值的代码public String maxQuery() &#123; return \"SELECT max(\" + time + \") FROM \" + table; &#125;//增量查询oracle语句public String buildQuery(String maxTime) &#123; if (customQuery == null) &#123; return \"SELECT \" + columnsToSelect + \" FROM \" + table + \" \" + \"WHERE \"+ time + \"&gt;=to_date('\" + currentIndex + \"','yyyy-mm-dd hh24:mi:ss') AND \" + time + \"&lt;to_date('\" + maxTime + \"','yyyy-mm-dd hh24:mi:ss') \" + \"order by \"+time+\" asc\"; &#125; else &#123; if (customQuery.contains(\"$@$\")) &#123; return customQuery.replace(\"$@$\", currentIndex) ; &#125; else &#123; return customQuery ; &#125; &#125;&#125; HibernateHelper修改executeQuery方法：123456789101112131415161718192021222324public List&lt;List&lt;Object&gt;&gt; executeQuery() throws InterruptedException, ParseException &#123; List&lt;List&lt;Object&gt;&gt; rowsList = new ArrayList&lt;List&lt;Object&gt;&gt;() ; Query query; if (!session.isConnected())&#123; resetConnection(); &#125; String sql = sqlSourceHelper.maxQuery(); LOG.info(\"sql \"+sql); List&lt;List&lt;Object&gt;&gt; max = session.createSQLQuery(sql).setResultTransformer(Transformers.TO_LIST).list(); String maxtime = max.get(0).get(0).toString().substring(0,19); query = session.createSQLQuery(sqlSourceHelper.buildQuery(maxtime)); try &#123; rowsList = query.setResultTransformer(Transformers.TO_LIST).list(); LOG.info(\"Current time is \"+sqlSourceHelper.getCurrentIndex()+\",and lasttime is \"+maxtime); LOG.info(\"Records count: \"+rowsList.size()); &#125;catch (Exception e)&#123; LOG.error(\"Exception thrown, resetting connection.\",e); resetConnection(); &#125; sqlSourceHelper.setCurrentIndex(maxtime); return rowsList;&#125;","categories":[{"name":"数据采集","slug":"数据采集","permalink":"https://1630425.github.io/categories/数据采集/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://1630425.github.io/tags/Flume/"}]},{"title":"为什么不用原生Spring-Cloud-Config","slug":"为什么不用原生Spring-Cloud-Config","date":"2019-01-12T11:56:19.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/68ef0897.html","link":"","permalink":"https://1630425.github.io/posts/68ef0897.html","excerpt":"","text":"引言近几年传统应用架构已经逐渐朝着微服务架构演进。那么随着业务的发展，微服务越来越庞大，此时服务配置的管理变得会复杂起来。为了方便服务配置文件统一管理，实时更新，配置中心应运而生。其实，所谓配置中心，就是将配置的数据放在某种存储介质中，该介质可以是 File(例如Git、Svn) Database(例如mysql、oracle) nosql Database(例如Redis、Memacache、MongoDb) 其他第三方中间件(例如Zookeeper) 那么配置中心可以简单理解为是封装了对这些介质进行操作的接口，供客户端拉取使用。 由于我们采用的是Spring-Boot的架构，因此当时自然而然会考虑到Spring-Cloud中提供的配置中心Spring-Cloud-Config，但是当时做完调研以后，觉得并不能直接用。因此，本文想来分享一下，原生Spring-Cloud-Config的配置中心的缺点，以及我们对Spring-Cloud-Config做了哪些改动。 正文OK，我们当时做配置中心的选型的时候。第一选择是Spring-Cloud- Config。Spring-Cloud-Config在存储介质的选择这块，基本上网上所有的文章都在推荐使用Git，即将配置文件放在Git中，服务端从Git中读取。其实官网上讲的最详细的配置，也是采取用Git作为存储介质。因此，我相信大部分读者在生产上也是用Git作为存储介质，搭配Spring-Cloud-Config使用。但是呢，博主认为以Git作为存储介质存在一些硬伤。 Git的权限控制是个坑Git的权限管理是说控制用户能不能Push或者Delete分支，或者能不能Push代码，而不是能不能访问某个目录的文件。对目录和文件的可读是Git的最基本要求，不可能做到针对目录级别的不可读。因此如果直接使用，会出现这样一种情形 不同团队之间可以互看对方配置！ 于是，可能会有如下情形发生 A团队同事A:&quot;小B,这个地方不懂怎么配？&quot;A团队同事B:&quot;去看看B团队的配置，直接贴过来。&quot;然后B团队就会发现自己的中间件里总是会多出一些莫名奇妙的数据！ 当然，你可以禁止研发直接登陆Git改配置。然后呢，基于Git研发一套配置管理系统，在上面做权限控制，但是又有几个公司这么做呢？因为，这可能带来第二个问题。 粒度问题将配置信息放在Git中，有一个致命问题: 粒度太粗了！你每次对一条配置发生crud的操作，其带来的影响是整个文件发生变动。如果将来我们需要对某条配置做灰度发布，基于Git来做是比较麻烦的，注意了，我没说不能做，只是比较麻烦。 那么，当时我们最理想的存储介质就是数据库，将配置信息放在数据库里有 两个好处 基于数据库开发一套配置管理系统，显然比基于git来开发容易的多！ 将配置放在数据库里，每条配置对应数据库的表中的一条记录。这么做粒度够细，针对某些重要的配置，做灰度发布，实现起来就容易很多。 因此，我们采用数据库作为存储介质。庆幸的是，这一点在Spring-Cloud-Config中是支持的。在该组件下，只需要设置 spring.profiles.active=jdbc 就能够激活jdbc模式。但是我们很快发现了一个更大的问题，也正是因为这个问题，我们不得以需要进行改写Spring-Cloud-Config。 Spring-Cloud-Config的刷新机制是个坑！ 因为一个配置中心应该要能够做到，配置发生改动的时候，项目能够自动感知，自动更新配置才对。在Spring-Cloud-Config中，这套机制是借助一些代码仓库（SVN、Github等）提供的Webhook机制加上Spring-Cloud-Bus来实现的。在Webhook中配置一个回调地址，刷新流程如下图所示 OK，那么问题又来了！(1)配置数据放在数据库中，数据库里没有Webhook这种东西啊，怎么做到实时刷新？(2)__Spring-Cloud-Config的这套刷新机制依赖于消息总线，依赖于消息队列，存在延迟的情况!且依赖于消息队列的可用性，系统的复杂度大大增加。如果生产环境上消息队列出问题了，我们的刷新功能就会受到影响！ 所以，笔者认为这套刷新机制并不是很尽如人意，需要进行修改。因此，我们很自然而然的想到了利用长轮询来改写Spring-Cloud-Config的刷新机制！ 长轮询是什么既然有长轮询，那必定有短轮询，我顺便讲讲短轮询是什么！假设我们有一个需求 在页面上要实时显示后台的库存数量！比如库存减少了，用户不需要进行刷新，页面上的数字自己会变化。 那么，如果采取短轮询就是在客户端(js)中不断访问后台，后台接到请求马上返回最新的库存数，然后刷新到这个页面当中。短轮询的缺点？很明显资源浪费。假设有几百人打开了该页面，就有几百个请求在不停的请求服务端，明显听着就不合理。 因此，自然就有了长轮询的出现!其实也很简单，客户端(js)依然是不断的去请求。但是呢，服务端不是马上返回。而是等待库存数量变化了再返回。大家知道，HTTP都有超时时间。如果在该时间内，依然没有变化，客户端将再次发起请求。 注意了，长短轮询对于客户端来说是没有区别的，就是不断的轮询。但是对于服务端，区别就比较大了。在短轮询情况下，服务端对于每次请求不管有没有变化都会立即返回结果。而长轮询情况下，如果有变化才会立即返回结果。而如果没有变化，服务端则不会再立即给客户端返回结果，直到超时为止。 怎么实现那么，我们在项目中采用Spring的DeferredResult来实现。在Servlet3.0以后引入了异步请求之后，Spring封装了一下提供了相应的支持，也就是DeferredResult，能够极大的提升吞吐量。可能有人对Servlet的异步化不熟，我大概介绍一下。我们平时常用的是同步Servlet，其执行流程如下图所示 缺点很明显啦，业务逻辑线程和servlet容器线程是同一个，一般的业务逻辑总得发生点IO，比如查询数据库，比如产生RPC调用，这个时候就会发生阻塞，而我们的servlet容器线程肯定是有限的，当servlet容器线程都被阻塞的时候我们的服务这个时候就会发生拒绝访问，从而吞吐量上不去！那么，你使用异步Servlet如下图所示 在异步Servlet中，业务线程有自己的线程池进行处理，并不会占用Tomcat中的线程，从而提升了吞吐量！ 那么，怎么利用DeferredResult怎么实现长轮询呢？流程如下(1)客户端和服务端建立TCP连接(2)客户端发起HTTP请求(3)服务端发起请求，监听60s内是否有配置发生变动(如何监听配置发生变动？)(4)如果没发生变动，给客户端返回304标志位，客户端继续发起请求(5)如果发生了变动，服务端会调用DeferredResult.setResult返回200状态码，客户端收到响应结果后，会发起请求获取变更后的配置信息。 最后一个问题:如何有效快速的监听出配置表的数据发生了变动？因为我们用的是mysql。这里有一个Mysql的自定义函数叫mysql-udf-http。具有http_get()、http_post()、http_put()、http_delete()四个函数，可以在MySQL数据库中利用HTTP协议进行REST相关操作。然后再和mysql的触发器结合起来用，可以实现在配置表发生变动的时候，主动通知我们的配置中心服务端。让服务端明白配置发生了变动！ 一个疑问采用长轮询技术来实现配置刷新，客户端和服务端需之间需要一直保持TCP连接进行通信。可能有些朋友会担心，到底服务端能撑多少的连接？可能觉得对性能有影响？这里给出参考配置使用了内存8G、4核的虚拟机约可以撑8000左右的连接！ 总结最后这套配置中心，我基于原有的Spring-Cloud-Config，改写其中的刷新机制，更加符合我们的业务场景!现已将改写思路说清，大家可以自行尝试！","categories":[{"name":"Spring-Cloud","slug":"Spring-Cloud","permalink":"https://1630425.github.io/categories/Spring-Cloud/"}],"tags":[]},{"title":"springboot2.x整合spring-data-jpa的问题","slug":"springboot2-x整合spring-data-jpa的问题","date":"2018-12-06T05:28:00.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/ff34caee.html","link":"","permalink":"https://1630425.github.io/posts/ff34caee.html","excerpt":"","text":"今天使用springboot整合spring-data-jpa遇到一些问题，直接使用JpaRepository的getOne()方法是会报错的。报错信息为：org.hibernate.LazyInitializationException: could not initialize proxy - no Session。在SpringBoot1.xx版本应该使用findOne()方法根据主键来查找对象。 这里是findOne和getOne的区别getOne API：返回对具有给定标识符的实体的引用。当我查询一个不存在的id数据时，直接抛出异常，因为它返回的是一个引用，简单点说就是一个代理对象。 findOne API：按ID查找实体。当我查询一个不存在的id数据时，返回的值是null. 详细对比参考这里。 但是新版本的JPA中，已经不存在用ID查找实体的findOne方法了，取而代之的是：findById().get()方法。 spring配置如下1234567891011121314spring: datasource: username: root password: 123456 url: jdbc:mysql://127.0.0.1:3306/jpa?useUnicode=true&amp;characterEncoding=UTF8 driver-class-name: com.mysql.cj.jdbc.Driver jpa: hibernate: #更新数据表结构 ddl-auto: update show-sql: true open-in-view: true 实体类123456789101112131415161718192021222324252627282930313233343536373839package com.wang.springboot06jap.entity;import javax.persistence.*;//使用JPA注解配置映射关系@Entity@Table(name = \"tbl_user\") //配置表名，若省略则默认表名是userpublic class User &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) //自增主键 private Integer id; @Column(name = \"lastName\",length = 20) //若省略就是属性名 private String username; private String email; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125;&#125; Reposity123public interface UserRepository extends JpaRepository&lt;User,Integer&gt; &#123;&#125; 调用JPAReposity的方法123456789101112131415@Controllerpublic class UserController &#123; @Autowired UserRepository userRepository; @GetMapping(\"/user/&#123;id&#125;\") @ResponseBody public User getUser(@PathVariable(\"id\") Integer id)&#123; User user = userRepository.findById(id).get(); // System.out.println(userRepository.getOne(id)); // return user; &#125;&#125;","categories":[],"tags":[]},{"title":"关于SpringJpa中getOne方法遇到延迟加载报错no Session的问题","slug":"关于SpringJpa中getOne方法遇到延迟加载报错no-Session的问题","date":"2018-10-11T02:02:00.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/c1ba8626.html","link":"","permalink":"https://1630425.github.io/posts/c1ba8626.html","excerpt":"","text":"报错如下：123org.hibernate.LazyInitializationException: could not initialize proxy - no Session... 看到报错信息推测如下：遇到延迟加载，session关闭了，导致不能得到有效信息。网上搜集了下资料，有关解释说，T getOne(ID id)依赖于EntityManager.getReference()执行实体延迟加载。https://stackoverflow.com/questions/24482117/when-use-getone-and-findone-methods-spring-data-jpa因为本人英文水平有限，所以用翻译软件，翻译其中一段：1234567891011121314151617181920212223242526272829虽然T getOne(ID id)javadoc声明（重点是我的）：返回对具有给定标识符的实体的引用。实际上，参考术语实际上是板，而JPA API没有指定任何getOne()方法。因此，了解Spring包装器的作用最好的方法是查看实现：@Overridepublic T getOne(ID id) &#123; Assert.notNull(id, ID_MUST_NOT_BE_NULL); return em.getReference(getDomainClass(), id);&#125;这em.getReference()是一个EntityManager声明为的方法：public &lt;T&gt; T getReference(Class&lt;T&gt; entityClass, Object primaryKey);幸运的是，EntityManagerjavadoc更好地定义了它的意图（重点是我的）：获取一个实例，其状态可能会被懒惰地取出。如果数据库中不存在请求的实例，则在首次访问实例状态时将引发EntityNotFoundException 。（在调用getReference时，允许持久性提供程序运行时抛出EntityNotFoundException。）除非在实体管理器打开时应用程序访问实例状态，否则应用程序不应期望实例状态在分离时可用。因此，调用getOne()可能会返回一个延迟获取的实体。这里，延迟提取不是指实体的关系，而是指实体本身。这意味着如果我们调用getOne()然后关闭Persistence上下文，则实体可能永远不会被加载，因此结果实际上是不可预测的。例如，如果代理对象是序列化的，则可以将null引用作为序列化结果获取，或者如果在代理对象上调用方法，LazyInitializationException则会引发异常，例如抛出。因此，在这种情况下，抛出EntityNotFoundException这是getOne()用于处理数据库中不存在的实例的主要原因，因为在实体不存在时可能永远不会执行错误情况。在任何情况下，为确保其加载，您必须在会话打开时操纵实体。您可以通过调用实体上的任何方法来完成此操作。或者更好的替代用途findById(ID id)而不是。 哈哈哈，有点乱，感兴趣的还是参考上面原文链接吧。 有关于对getOne和findOne区别的说明12getOne API：返回对具有给定标识符的实体的引用。当我查询一个不存在的id数据时，直接抛出异常，因为它返回的是一个引用，简单点说就是一个代理对象。findOne API：按ID查找实体。当我查询一个不存在的id数据时，返回的值是null. 但是新版本的JPA中，已经不存在用ID查找实体的findOne方法了，取而代之的是：**findById().get()**方法。而解决方法中，有一种是说在application.properites中加上如下配置：1spring.jpa.properties.hibernate.enable_lazy_load_no_trans=true 但是 我试了下没有效果~~，不知哪里出了问题。所有还是采用**findById().get()**来代替吧。","categories":[],"tags":[]},{"title":"Kudu设计原理初探","slug":"Kudu设计原理初探","date":"2018-08-26T15:00:00.000Z","updated":"2019-06-05T03:03:41.000Z","comments":true,"path":"posts/92eadbe.html","link":"","permalink":"https://1630425.github.io/posts/92eadbe.html","excerpt":"","text":"如何在一个系统中融合OLTP型随机读写能力与OLAP型分析能力，Kudu提供了优秀的设计思路。本文主要从Kudu的设计论文着手，结合与HBase的对比分析，来初步揭示Kudu的设计原理，部分设计在最新的Kudu版本中可能已经过时，但最初的设计思想依然值得借鉴。 Kudu的设计初衷在介绍Kudu是什么之前，还是先简单的说一下现存系统针对结构化数据存储与查询的一些痛点问题，结构化数据的存储，通常包含如下两种方式： 静态数据通常以Parquet/Carbon/Avro形式直接存放在HDFS中，对于分析场景，这种存储通常是更加适合的。但无论以哪种方式存在于HDFS中，都难以支持单条记录级别的更新，随机读取也并不高效。 可变数据的存储通常选择HBase或者Cassandra，因为它们能够支持记录级别的高效随机读写。但这种存储却并不适合离线分析场景，因为它们在大批量数据获取时的性能较差（针对HBase而言，有两方面的主要原因：一是HFile本身的结构定义，它是按行组织数据的，这种格式针对大多数的分析场景，都会带来较大的IO消耗，因为可能会读取很多不必要的数据，相对而言Parquet格式针对分析场景就做了很多优化。 二是由于HBase本身的LSM-Tree架构决定的，HBase的读取路径中，不仅要考虑内存中的数据，同时要考虑HDFS中的一个或多个HFile，较之于直接从HDFS中读取文件而言，这种读取路径是过长的）。 可以看出，如上两种存储方式，都存在明显的优缺点： 直接存放于HDFS中，适合离线分析，却不利于记录级别的随机读写。 直接将数据存放于HBase/Cassandra中，适合记录级别的随机读写，对离线分析却不友好。但在很多实际业务场景中，两种场景时常是并存的。我们的通常做法有如下几种： 1. 数据存放于HBase中，对于分析任务，基于Spark/Hive On HBase进行，性能较差。2. 对于分析性能要求较高的，可以将数据在HDFS/Hive中多冗余存放一份，或者，将HBase中的数据定期的导出成Parquet/Carbon格式的数据。 明显这种方案对业务应用提出了较高的要求，而且容易导致在线数据与离线数据之间的一致性问题。Kudu的设计，就是试图在OLAP与OLTP之间，寻求一个最佳的结合点，从而在一个系统的一份数据中，既能支持OLTP型实时读写能力又能支持OLAP型分析。另外一个初衷，在Cloudera发布的《Kudu: New Apache Hadoop Storage for Fast Analytics on Fast Data》一文中有提及，Kudu作为一个新的分布式存储系统期望有效提升CPU的使用率，而低CPU使用率恰是HBase/Cassandra等系统的最大问题。下面的章节中，主要从论文所揭示的内容来解读Kudu的设计原理。 Kudu的原理介绍Kudu自身的架构，部分借鉴了Bigtable/HBase/Spanner的设计思想。论文的作者列表中，有几位是HBase社区的Committer/PBC成员，因此，在论文中也能很深刻的感受到HBase对Kudu设计的一些影响，因此，在本文的多个地方都有谈及Kudu与HBase在设计上的异同。 表与SchemaKudu设计是面向结构化存储的，因此，Kudu的表，需要用户在建表时定义它的Schema信息，这些Schema信息包含：列定义（含类型），Primary Key定义（用户指定的若干个列的有序组合）。数据的唯一性，依赖于用户所提供的Primary Key中的Column组合的值的唯一性。 Kudu提供了Alter命令来增删列，但位于Primary Key中的列是不允许删除的。Kudu当前并不支持二级索引。 APIKudu提供了Java/C++两种语言的API（尽管也提供了Python API，但尚处于Experimental阶段）。通过这些API，可以进行如下一些操作： 1. Insert/Update/Delete2. 批量数据导入/更新操作3. Scan(可支持简单的Filter) 事务与一致性模型Kudu仅仅提供单行事务，也不支持多行事务。这一点与HBase是相似的。但在数据一致性模型上，与HBase有较大的区别。 Kudu提供了如下两种一致性模型： 1. Snapshot Consistency这是Kudu中的默认一致性模型。在这种模型中，只保证一个客户端能够看到自己所提交的写操作，而并不保障全局的（跨多个客户端的）事务可见性。 2. External Consistency最早提出External Consistency机制的，应该是在Google的Spanner论文中。传统关系型数据库中的两阶段提交机制，需要两回合通信，这过程中带来的代价是较高的，但同时这过程中的复杂的锁机制也可能会带来一些可用性问题。一个更好的实现分布式事务/一致性的思路，是基于一个全局发布的Timestamp机制。Spanner提出了Commit-wait的机制，来保障全局事务的有序性：如果一个事务T1的提交先于另外一个事务T2的开始，则T1的Timestamp要小于T2的TimeStamp。我们知道，在分布式系统中，是很难于做这样的承诺的。在HBase中，我们可以想象，如果所有RegionServer中的SequenceID发布自同一个数据源，那么，HBase的很多事务性问题就迎刃而解了，然后最大的问题在于这个全局的SequenceID数据源将会是整个系统的性能瓶颈点。回到External Consistency机制，Spanner是依赖于高精度与可预见误差的本地时钟(TrueTime API)实现的(即需要一个高可靠和高精度的时钟源，同时，这个时钟的误差是可预见的。感兴趣的同学可以阅读Spanner论文，这里不赘述)。Kudu中提供了另外一种思路来实现External Consistency,基于Timestamp扩散机制，即，多个客户端可相互通信来告知彼此所提交的Timestamp值，从而保障一个全局的顺序。这种机制也是相对较为复杂的。与Spanner类似，Kudu不允许用户自定义用户数据的Timestamp，但在HBase中却是不同，用户可以发起一次基于某特定Timestamp的查询。 Kudu的架构Kudu也采用了Master-Slave形式的中心节点架构，管理节点被称作Kudu Master，数据节点被称作Tablet Server（可对比理解HBase中的RegionServer角色）。一个表的数据，被分割成1个或多个Tablet，Tablet被部署在Tablet Server来提供数据读写服务。Kudu Master在Kudu集群中，发挥如下的一些作用：1. 用来存放一些表的Schema信息，且负责处理建表等请求。2. 跟踪管理集群中的所有的Tablet Server，并且在Tablet Server异常之后协调数据的重部署。3. 存放Tablet到Tablet Server的部署信息。Tablet与HBase中的Region大致相似，但存在如下一些明显的区别点： 1. Tablet包含两种分区策略，一种是基于Hash Partition方式，在这种分区方式下用户数据可较均匀的分布在各个Tablet中，但原来的数据排序特点已被打乱。另外一种是基于Range Partition方式，数据将按照用户数据指定的有序的Primary Key Columns的组合String的顺序进行分区。而HBase中仅仅提供了一种按用户数据RowKey的Range Partition方式。2. 一个Tablet可以被部署到了多个Tablet Server中。在HBase最初的架构中，一个Region只能被部署在一个RegionServer中，它的数据多副本交由HDFS来保障。从1.0版本开始，HBase有了Region Replica（HBASE-10070）特性，该特性允许将一个Region部署在多个RegionServer中来提升读取的可用性，但多Region副本之间的数据却不是实时同步的。 Kudu的底层数据模型Kudu的底层数据文件的存储，未采用HDFS这样的较高抽象层次的分布式文件系统，而是自行开发了一套可基于Table/Tablet/Replica视图级别的底层存储系统。这套实现基于如下的几个设计目标： 可提供快速的列式查询。 可支持快速的随机更新 可提供更为稳定的查询性能保障。为了实现如上目标，Kudu参考了一种类似于Fractured Mirrors的混合列存储架构。Tablet在底层被进一步细分成了一个称之为RowSets的单元： MemRowSets可以对比理解成HBase中的MemStore, 而DiskRowSets可理解成HBase中的HFile。MemRowSets中的数据按照行试图进行存储，数据结构为B-Tree。MemRowSets中的数据被Flush到磁盘之后，形成DiskRowSets。 DisRowSets中的数据，按照32MB大小为单位，按序划分为一个个的DiskRowSet。DiskRowSet中的数据按照Column进行组织，与Parquet类似。这是Kudu可支持一些分析性查询的基础。每一个Column的数据被存储在一个相邻的数据区域，而这个数据区域进一步被细分成一个个的小的Page单元，与HBase File中的Block类似，对每一个Column Page可采用一些Encoding算法，以及一些通用的Compression算法。既然可对Column Page可采用Encoding以及Compression算法，那么，对单条记录的更改就会比较困难了。前面提到了Kudu可支持单条记录级别的更新/删除，是如何做到的？与HBase类似，也是通过增加一条新的记录来描述这次更新/删除操作的。一个DiskRowSet包含两部分数据：基础数据(Base Data)，以及变更数据(Delta Stores)。更新/删除操作所生成的数据记录，被保存在变更数据部分。 从上图（源自Kudu的源工程文件）来看，Delta数据部分应该包含REDO与UNDO两部分，这里的REDO与UNDO与关系型数据库中的REDO与UNDO日志类似（在关系型数据库中，REDO日志记录了更新后的数据，可以用来恢复尚未写入Data File的已成功事务更新的数据。 而UNDO日志用来记录事务更新之前的数据，可以用来在事务失败时进行回滚），但也存在一些细节上的差异： REDO Delta Files包含了Base Data自上一次被Flush/Compaction之后的变更值。REDO Delta Files按照Timestamp顺序排列。 UNDO Delta Files包含了Base Data自上一次Flush/Compaction之前的变更值。这样才可以保障基于一个旧Timestamp的查询能够看到一个一致性视图。UNDO按照Timestamp倒序排列。 数据读写流程写数据的流程，如下图所示： Kudu不允许用户数据的Primary Key重复，因此，在Tablet内部写入数据之前，需要先从已有的数据中检查当前新写入的数据的Primary Key是否已经存在，尽管在DiskRowSets中增加了BloomFilter来提升这种判断的效率，但可以预见，Kudu的这种设计将会明显增大写入的时延。数据一开始先存放于MemRowSets中，待大小超出一定的阈值之后，再Flush成DiskRowSets。这部分已经在图4中有详细的介绍。随着Flush次数的不断增加，生成的DiskRowSets也会不断的增多，在Kudu内部也存在一个Compaction流程，这样可以将已经存在的多个存在Primary Key交集的DiskRowSets重新排序而生成一个新的DiskRowSets。如下图所示：读数据的流程，既要考虑存在于内存中的MemRowSets,又要读取位于磁盘中的一个或多个DiskRowSets，在Scanner的高层抽象中，应该与HBase类似。如下重点提一些细节的优化点： 通过Scan的范围，与每一个DiskRowSets中的Primary Key Range进行对比，可以首先过滤掉一些不必要参与此次Scan的DiskRowSets。 Delta Store部分，针对记录级别的更改，记录了Base Data中对应原始数据的Offset。这样，在判断一条记录是否存在更改的记录时，将会更加的快速。 由于DiskRowSets的底层文件是按照列组织的，基于一些列的条件进行过滤查询时，可以优先过滤掉一些不必要的Primary Keys。Kudu并不会在一开始读取的时候就将一行数据的所有列读取出来，而是先读取与过滤条件相关的列，通过将这些列与查询条件匹配之后，再来决定是否去读取符合条件的行中的其它的列信息。这样可以节省一些磁盘IO。这就是Kudu所提供的Lazy Materialization特性。 Raft模型Kudu的多副本之间的数据共识协议采用了Raft协议，Raft是比Paxos更容易理解且更简单的一种一致性协议。关于Raft的更多信息，请参考：https://raft.github.io/ Kudu与HBase的区别这里再总结一下Kudu与HBase的一些大的区别点： Kudu的数据分区方式相对多样化，而HBase较单一。 Kudu的Tablet自身具备多副本机制，而HBase的Region依赖于底层HDFS的多副本机制。 Kudu底层直接采用本地文件系统， 而HBase依赖于HDFS。 Kudu的底层文件格式采用了类似于Parquet的列式存储格式，而HBase的底层HFile文件却是按行来组织的。 Kudu关于底层的Flush任务以及Compaction任务，能够结合忙时或者闲时进行自动的调整。HBase还尚不具备这种调度能力。 Kudu的Compaction无Minor/Major的区分，限制每一次Compaction的IO总量在128MB大小，因此，并不存在长久执行的Compaction任务。 Compaction是按需进行的，例如，如果所有的写入都是顺序写入，则将不会触发Compaction。 Kudu的设计，既兼顾了分析型的查询能力，又兼顾了随机读写能力，这样，势必也会付出一些代价。 例如，写入数据时关于Primary Key唯一性的限制，就要求写入前要检查对应的Primary Key是否已经存在，这样势必会增大写入的时延。而底层尽管采用了类似于Parquet的列式文件设计，但与HBase类似的冗长的读取路径，也会对分析性的查询带来一些影响。另外，这种设计在整行读取时，也会付出较高的代价。 Kudu与现有系统的对接Kudu提供了与如下一些系统的对接： MapReduce: 提供针对Kudu用户表的Input以及Output任务对接。 Spark: 提供与Spark SQL以及DataFrames的对接。 Impala: Kudu自身未提供Shell以及SQL Parser，所以，它的SQL能力源自与Impala的集成。在这些集成中，能够很好的感知Kudu表数据的本地性信息，能够充分利用Kudu所提供的过滤器对查询进行优化，同时，Impala本身的DDL/DML语法针对Kudu也做了一些扩展。可以想象，Cloudera在Impala与Kudu的集成上，一定会有更多的发力点。 Kudu的适用场景Todd Lipcon在Strata+Hadoop World 2015大会上所提供的主题为《Kudu: Resolving transactional and analytic trade-offs in Hadoop》的演讲中，这样子描述Kudu的适用场景： Kudu Benchmark数据解析如下是对Kudu WhitePage中所提供的一些Benchmark性能测试数据的简单解析(详细的结果请参考论文的第6章节)：1. 基于TPC-H测试标准：针对Impala On Parquet以及Impala On Kudu做了对比测试，Impala On Kudu的平均性能比Impala On Parquet提升了31%。这是由于Kudu所提供的Lazy Meterialization特性以及对对CPU效率的提升而带来的成果。2. Impala-Kudu与Phoenix-HBase的对比：测试使用到了TPC-H中的lineitem一表，共导入了62GB的CSV格式的数据。在导入Phoenix时使用了Phoenix所提供的CsvBulkLoadTool工具。测试时的一些配置信息如下所示： 为Phoenix表划分了100个Hash Partitions。为Kudu创建了100个Tablets。 HBase采用默认的Block Cache策略，为每一个RegionServer配置了9.6GB的Cache内存。而Kudu配置了1GB的Block Cache的进程内存，但同时还依赖于操作系统的Buffer。 HBase表中采用了FAST_DIFF的Block Encoding算法，未启用任何压缩。数据导入到HBase中之后，主动触发了一次Major Compaction，来确保数据的本地化率。62GB原始数据导入到HBase中之后的总大小约为570GB（这是由于未启用Compression压缩，同时，由于多个列都是独立存在的带来的膨胀导致），而导入到Kudu中之后的大小约为227GB。如下是相应的对比测试场景以及对比结果：除了基于Key值的整行数据的查询性能，Phoenix有明显的优势以外，其它的基于整表扫描，或者是基于一些列的查询，Impala-Kudu是有明显的优势的。基于Scan + Filter的查询，HBase本身就不擅长。3. 随机读写能力的对比如下是对比测试的一些场景：如下是对比测试的结果：关于加载以及Zipfian分布模式下，HBase的优势更加明显，当前Kudu也正在做关于Zipfian分布模式下的优化（KUDU-749），而在Uniform模式下，HBase的优势稍弱。整体来看，在随机读写上，Kudu的设计较之HBase而言，存在一些劣势，这是为了兼顾分析型查询所付出的一些代价。参考信息[1] Kudu: New Apache Hadoop Storage for Fast Analytics on Fast Data[2] http://getkudu.io/kudu.pdf[3] Kudu: Resolving transactional and analytic trade-offs in Hadoop[4] Spanner: Google’s Globally-Distributed Database[5] https://kudu.apache.org/docs/ 注： 本文中大多数图源自Kudu论文以及参考信息中的相关内容。","categories":[],"tags":[{"name":"Kudu","slug":"Kudu","permalink":"https://1630425.github.io/tags/Kudu/"}]},{"title":"Kudu介绍","slug":"KUDU介绍","date":"2018-08-25T14:30:00.000Z","updated":"2019-06-05T03:03:40.996Z","comments":true,"path":"posts/8920857f.html","link":"","permalink":"https://1630425.github.io/posts/8920857f.html","excerpt":"","text":"前言近两年，KUDU 在大数据平台的应用越来越广泛。在阿里、小米、网易等公司的大数据架构中，KUDU 都有着不可替代的地位。本文通过分析 KUDU 的设计， 试图解释为什么 KUDU 会被广泛应用于大数据领域，因为还没有研究过 KUDU 的代码，下面的介绍是根据 KUDU 的论文和网上的一些资料学习自己理解所得，如有不实之处，劳请指正。 背景在 KUDU 之前，大数据主要以两种方式存储： 静态数据：以 HDFS 引擎作为存储引擎，适用于高吞吐量的离线大数据分析场景。这类存储的局限性是数据无法进行随机的读写。 动态数据：以 HBase、Cassandra 作为存储引擎，适用于大数据随机读写场景。这类存储的局限性是批量读取吞吐量远不如 HDFS，不适用于批量数据分析的场景。 从上面分析可知，这两种数据在存储方式上完全不同，进而导致使用场景完全不同，但在真实的场景中，边界可能没有那么清晰，面对既需要随机读写，又需要批量分析的大数据场景，该如何选择呢？这个场景中，单种存储引擎无法满足业务需求，我们需要通过多种大数据工具组合来满足这一需求，一个常见的方案是：如上图所示，数据实时写入 HBase，实时的数据更新也在 HBase 完成，为了应对 OLAP 需求，我们定时（通常是 T+1 或者 T+H）将 HBase 数据写成静态的文件（如：Parquet）导入到 OLAP 引擎（如：HDFS）。这一架构能满足既需要随机读写，又可以支持 OLAP 分析的场景，但他有如下缺点： 架构复杂。从架构上看，数据在 HBase、消息队列、HDFS 间流转，涉及环节太多，运维成本很高。并且每个环节需要保证高可用，都需要维护多个副本，存储空间也有一定的浪费。最后数据在多个系统上，对数据安全策略、监控等都提出了挑战。 时效性低。数据从 HBase 导出成静态文件是周期性的，一般这个周期是一天（或一小时），在时效性上不是很高。 难以应对后续的更新。真实场景中，总会有数据是「延迟」到达的。如果这些数据之前已经从 HBase 导出到 HDFS，新到的变更数据就难以处理了，一个方案是把原有数据应用上新的变更后重写一遍，但这代价又很高。 为了解决上述架构的这些问题，KUDU 应运而生。KUDU 的定位是 「Fast Analytics on Fast Data」，是一个既支持随机读写、又支持 OLAP 分析的大数据存储引擎。从上图可以看出，KUDU 是一个「折中」的产品，在 HDFS 和 HBase 这两个偏科生中平衡了随机读写和批量分析的性能。从 KUDU 的诞生可以说明一个观点：底层的技术发展很多时候都是上层的业务推动的，脱离业务的技术很可能是「空中楼阁」。 概览数据模型KUDU 的数据模型与传统的关系型数据库类似，一个 KUDU 集群由多个表组成，每个表由多个字段组成，一个表必须指定一个由若干个（&gt;=1）字段组成的主键，如下图：KUDU 表中的每个字段是强类型的，而不是 HBase 那样所有字段都认为是 bytes。这样做的好处是可以对不同类型数据进行不同的编码，节省空间。同时，因为 KUDU 的使用场景是 OLAP 分析，有一个数据类型对下游的分析工具也更加友好。 核心 APIKUDU 的对外 API 主要分为写跟读两部分。其中写包括：Insert、Update、Delete，所有写操作都必须指定主键；读 KUDU 对外只提供了 Scan 操作，Scan 时用户可以指定一个或多个过滤器，用于过滤数据。 一致性模型跟大多数关系型数据库一样，KUDU 也是通过 MVCC（Multi-Version Concurrency Control）来实现内部的事务隔离。KUDU 默认的一致性模型是 Snapshot Consistency，即客户端可以一致的访问到某个时间点的一个快照。如果有更高的外部一致性（external consistency）需求，KUDU 目前还没有实现，不过 KUDU 提供了一些设计方案。这里先介绍下外部一致性，它是指：多个事务并发执行达到串行效果，并且保证修改时间戳严格按照事务发生先后顺序，即如果有先后两个事务 A、B， A 发生在 B 之前，那么对于客户端来说，要么看到 A，要么看到 A、B，不会只看到 B 而看不到 A。KUDU 提供了两个实现外部一致性的方案： 方案一：在各个 Client 之间传播带有时间戳的 token，大致思路是 Client 提交完一个写请求后，生成一个带时间戳的 token，然后把这个 token 传播给其他客户端，其他客户端请求的时候可以带上这个 token。 方案二：类似 Google Spanner 的方案，通过 commit-wait 机制实现外部一致性。 这里我们衍生介绍下 Google Spanner 是如何实现分布式事务的外部一致性的。首先我们先明确下分布式事务外部一致性这个问题的由来。首先，在数据库中，我们出于性能考虑，一般我们对读不加排他锁，只对写进行加排他锁，这就会带来一个问题，数据在读取的时候可能正在被修改，导致同一事务中多次读取到的数据可能不一致，为了解决这个问题，我们引入了 MVCC。在单机系统中，通过 MVCC 就能解决外部一致性问题，因为每个事务都有一个在本机生成的一个时间戳，根据事务的时间戳先后，我们就能判断出事务发生的先后顺序。但是在分布式系统中，要实现外部一致性就没有那么简单了，核心问题是事务在不同的机器上执行，而不同机器的本地时钟是有误差的，因此就算是真实发生的事务顺序是 A-&gt;B，但是在事务持久化的时候记录的时间戳可能是 B &lt; A，这时如果一个事务 C 来读取数据，可能只读到 B 而没有读到 A。从上面的分析我们可以发现，分布式系统中保证事务的外部一致性的核心是一个精确的事务版本（时间戳），而最大的难点也在这里，计算机上的时钟不是一个绝对精确的时间，它跟标准时间是有一定的随机的误差的，导致分布式系统中不同机器之间的时间有偏差。Google Spanner 的解决思路是把不同机器的误差时间控制在一个很小的确定的范围内，再配合 commit-wait 机制来实现外部一致性。 控制时间误差的方案称为 TrueTime，它通过硬件（GPS 和原子钟）和软件结合，保证获取到的时间在较小误差（±4ms）内绝对正确，具体的实现这里就不展开了，有兴趣的同学可以自行找资料研究。TrueTime 对外只提供 3 个 API，如下：这里最主要的 API 是 TT.now()，它范围当前绝对精确时间的上下界，表示当前绝对精确时间在 TT.now().earliest 和 TT.now().latest 之间。 有了一个有界误差的 TrueTime 后，就可以通过 commit-wait 机制来实现外部一致性了，具体的方案如下：如上图所示，在一个事务开始获取锁执行后，生成事务的时间版本 s=TT.now().latest，然后开始执行事务的具体操作，但是一个事务的结束并不只由事务本身的时间消耗决定，它还要保证后续的事务时间版本不会早于自己，因此，事务需要等待直到 TT.now().earliest &gt; s 后，才算真正结束。根据整个 commit-wait 过程我们可以知道，整个事务提交过程需要等待 2 倍的平均误差时间（ε），TrueTime 的平均误差时间是 4 ms，因此一次 commit-wait 需要至少 8 ms。 之前我们提到，KUDU 也借鉴 Spanner 使用 commit-wait 机制实现外部一致性，但是 commit-wait 强依赖于 TrueTime，而 TrueTime 需要各种昂贵的硬件设备支持，目前 KUDU 通过纯软件算法的方式来实现时钟算法，为 HybridTime，但这个方案时间误差较大，考虑到 commit-wait 需要等待 2ε 时间，因此误差一大实际场景使用限制就很多了。 架构整体架构KUDU 中存在两个角色 Mater Server：负责集群管理、元数据管理等功能 Tablet Server：负责数据存储，并提供数据读写服务 为了实现分区容错性，跟其他大数据产品一样，对于每个角色，在 KUDU 中都可以设置特定数量（一般是 3 或 5）的副本。各副本间通过 Raft 协议来保证数据一致性。Raft 协议与 ZAB 类似，都是 Paxos 协议的工程简化版本，具体细节有兴趣的同学可以搜索相关资料学习。KUDU Client 在与服务端交互时，先从 Master Server 获取元数据信息，然后去 Tablet Server 读写数据，如下图： 数据分区策略与大多数大数据存储引擎类似，KUDU 对表进行横向分区，KUDU 表会被横向切分存储在多个 tablets 中。不过相比与其他存储引擎，KUDU 提供了更加丰富灵活的数据分区策略。 一般数据分区策略主要有两种，一种是 Range Partitioning，按照字段值范围进行分区，HBase 就采用了这种方式，如下图：Range Partitioning 的优势是在数据进行批量读的时候，可以把大部分的读变成同一个 tablet 中的顺序读，能够提升数据读取的吞吐量。并且按照范围进行分区，我们可以很方便的进行分区扩展。其劣势是同一个范围内的数据写入都会落在单个 tablet 上，写的压力大，速度慢。 另一种分区策略是 Hash Partitioning，按照字段的 Hash 值进行分区，Cassandra 采用了这个方式，见下图：与 Range Partitioning 相反，由于是 Hash 分区，数据的写入会被均匀的分散到各个 tablet 中，写入速度快。但是对于顺序读的场景这一策略就不太适用了，因为数据分散，一次顺序读需要将各个 tablet 中的数据分别读取并组合，吞吐量低。并且 Hash 分区无法应对分区扩展的情况。 各种分区策略的优劣对比见下图：既然各分区策略各有优劣，能否将不同分区策略进行组合，取长补短呢？这也是 KUDU 的思路，KUDU 支持用户对一个表指定一个范围分区规则和多个 Hash 分区规则，如下图： 存储存储设计目标 快速的列扫描 低延迟的随机更新 稳定的性能表现存储方式KUDU 是一个列式存储的存储引擎，其数据存储方式如下： 列式存储的数据库很适合于 OLAP 场景，其特点如下： 优势1. 查询少量列时 IO 少，速度快2. 数据压缩比高3. 便于查询引擎性能优化：延迟物化、直接操作压缩数据、向量化执行 劣势1. 查询列太多时性能下降（KUDU 建议列数不超过 300 ）2. 不适合 OLTP 场景 存储实现与其他大数据存储引擎类似，KUDU 的存储也是通过 LSM 树（Log-Structured Merge Tree）来实现的。KUDU 的最小存储单元是 RowSets，KUDU 中存在两种 RowSets：MemRowSets、DiskRowSets，数据先写内存中的 MemRowSet，MemRowSet 满了后刷到磁盘成为一个 DiskRowSet，DiskRowSet 一经写入，就无法修改了。见下图：当然上面只是最粗粒度的一个写入过程，为了解释 KUDU 的为什么既能支持随机读写，又能支持大数据量的 OLAP 分析，我们需要更进一步进行解剖分析。我们需求探究的主要两个问题是： 如何应对数据变更？ 如何优化读写性能以满足 OLAP 场景？ 应对数据变更首先上面我们讲了，DiskRowSet 是不可修改了，那么 KUDU 要如何应对数据的更新呢？在 KUDU 中，把 DiskRowSet 分为了两部分：base data、delta stores。base data 负责存储基础数据，delta stores负责存储 base data 中的变更数据。整个数据更新方案如下：如上图所示，数据从 MemRowSet 刷到磁盘后就形成了一份 DiskRowSet（只包含 base data），每份 DiskRowSet 在内存中都会有一个对应的 DeltaMemStore，负责记录此 DiskRowSet 后续的数据变更（更新、删除）。DeltaMemStore 内部维护一个 B-树索引，映射到每个 row_offset 对应的数据变更。DeltaMemStore 数据增长到一定程度后转化成二进制文件存储到磁盘，形成一个 DeltaFile，随着 base data 对应数据的不断变更，DeltaFile 逐渐增长。 优化读写性能首先我们从 KUDU 的 DiskRowSet 数据结构上分析：从上图可知，在具体的数据（列数据、变更记录）上，KUDU 都做了 B- 树索引，以提高随机读写的性能。在 base data 中，KUDU 还针对主键做了好几类索引（实际上由于 delta store 只记录变更数据，base data 中对主键的索引即本 DiskRowSet 中全局的主键索引）： 主键范围索引：记录本 DiskRowSet 中主键的范围，用于粗粒度过滤一些主键范围 布隆过滤器：通过主键的布隆过滤器来实现不存在数据的过滤 主键索引：要精确定位一个主键是否存在，以及具体在 DiskRowSet 中的位置（即：row_offset），通过以 B-树为数据结构的主键索引来快速查找。 随着时间的推移，KUDU 中的小文件会越来越多，主要包括各个 DiskRowSet 中的 base data，还有每个 base data 对应的若干份 DeltaFile。小文件的增多会影响 KUDU 的性能，特别是 DeltaFile 中还有很多重复的数据。为了提高性能，KUDU 会进行定期 compaction，compaction 主要包括两部分： DeltaFile compaction：过多的 DeltaFile 影响读性能，定期将 DeltaFile 合并回 base data 可以提升性能。在通常情况下，会发生频繁变更的字段是集中在少数几个字段中的，而 KUDU 是列式存储的，因此 KUDU 还在 DeltaFile compaction 时做了优化，文件合并时只合并部分变更列到 base data 中对应的列。 DiskRowSet compaction：除了 DeltaFile，定期将 DiskRowSet 合并也能提升性能，一个原因是合并时我们可以将被删除的数据彻底的删除，而且可以减少同样 key 范围内数据的文件数，提升索引的效率。 当用户的查询存在列的过滤条件时，KUDU 还可以在查询时进行 延迟物化（Lazy Materialization ）来提升性能。举例说明，现在我们有这样一张表：用户的 SQL 是这样的：1SELECT * FROM tb WHERE sex=‘男’ ADN age &gt; 20 KUDU 中数据查询过程是这样的： 1. 扫描 sex 列，过滤出要查询的行 [1,3]2. 扫描 age 列，过滤出要查询的行 [3,4]3. 过滤条件相交，得到 34. 真正读取 id=3 行对应的所有列信息，组装 上述查询中，KUDU 真正需要去物理读取的数据只有 id=3 这一行，这样就减少了 IO 数量。 读写过程数据写过程如上图，当 Client 请求写数据时，先根据主键从 Mater Server 中获取要访问的目标 Tablets，然后到依次对应的 Tablet 获取数据。因为 KUDU 表存在主键约束，所以需要进行主键是否已经存在的判断，这里就涉及到之前说的索引结构对读写的优化了。一个 Tablet 中存在很多个 RowSets，为了提升性能，我们要尽可能地减少要扫描的 RowSets 数量。首先，我们先通过每个 RowSet 中记录的主键的（最大最小）范围，过滤掉一批不存在目标主键的 RowSets，然后在根据 RowSet 中的布隆过滤器，过滤掉确定不存在目标主键的 RowSets，最后再通过 RowSets 中的 B-树索引，精确定位目标主键是否存在。如果主键已经存在，则报错（主键重复），否则就进行写数据（写 MemRowSet）。 数据更新过程数据更新的核心是定位到待更新数据的位置，这块与写入的时候类似，就不展开了，等定位到具体位置后，然后将变更写到对应的 delta store 中。 数据读过程如上图，数据读取过程大致如下：先根据要扫描数据的主键范围，定位到目标的Tablets，然后读取 Tablets 中的 RowSets。在读取每个 RowSet 时，先根据主键过滤要 scan 范围，然后加载范围内的 base data，再找到对应的 delta stores，应用所有变更，最后 union 上 MenRowSet 中的内容，返回数据给 Client。 应用案例这里介绍一个小米使用 KUDU 的案例。具体的业务场景是这样的：收集手机App和后台服务发送的 RPC 跟踪事件数据，然后构建一个服务监控和问题诊断的工具。 高写入吞吐：每天大于200亿条记录为了能够尽快定位和解决问题，要求系统能够查询最新的数据并能快速返回结果为了方便问题诊断，要求系统能够查询/搜索明细数据（而不只是统计信息）在使用 KUDU 前，小米的架构是这样的：一部分源系统数据是通过Scribe（日志聚合系统）把数据写到HDFS，另一部分源系统数据直接写入HBase。然后通过Hive/MR/Spark作业把两部分数据合并，给离线数仓和 OLAP 分析。 在使用 KUDU 后，架构简化成了：从上图我们可以看到，所有的数据存储都集中到的 KUDU 一个上，减少了整体的架构复杂度，同时，也大大提升了实时性。","categories":[],"tags":[{"name":"Kudu","slug":"Kudu","permalink":"https://1630425.github.io/tags/Kudu/"}]},{"title":"HBase对比HDFS","slug":"HBase对比HDFS","date":"2018-08-24T13:16:54.000Z","updated":"2019-06-05T03:03:40.992Z","comments":true,"path":"posts/21e5a7a5.html","link":"","permalink":"https://1630425.github.io/posts/21e5a7a5.html","excerpt":"","text":"什么是HDFS(Hadoop分布式文件系统)HDFS允许以分布式和冗余方式存储大量数据。HDFS组件• NameNode• DataNodeNameNode：NameNode可以被视为系统的管理者。它维护系统文件树以及系统中存在的所有文件和目录的元数据。其中“命名空间镜像(Namespace image)”和“编辑日志”用于存储元数据信息。 Namenode包含所有数据节点的数据块信息，但是，它不会持久存储数据节点数据块位置信息。系统启动时，每次从数据节点重建此信息。DataNode:是集群中的从属者，提供实际存储。它主要负责为客户提供读写请求服务。 什么是HbaseHbase是一个可以运行在Hadoop集群上的NoSQL数据库。Hbase组件• Hbase Master• Region Server• Region• ZookeeperHbase的架构图如下图所示： Hbase与HDFS对比总结一下：什么时候选用Hbase，什么场景使用HDFS进行存储？1. 对于经常需要修改原有的数据的场景使用Hbase进行存储；2. 对于性能要求不高且只需要支持单条数据查询或者小批量数据进行查询，两者均可；3. 对于需要经常进行全表扫描进行大批量的查询的选择HDFS； 那么有没有一种存储方式既能满足实时的更新，又能满足大量的数据分析工作，这时候可以考虑一下使用kudu。","categories":[],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://1630425.github.io/tags/HBase/"},{"name":"HDFS","slug":"HDFS","permalink":"https://1630425.github.io/tags/HDFS/"}]},{"title":"选择Kong作为你的API网关","slug":"选择Kong作为你的API网关","date":"2018-07-12T12:28:15.000Z","updated":"2019-06-05T03:03:41.060Z","comments":true,"path":"posts/c6cc21ed.html","link":"","permalink":"https://1630425.github.io/posts/c6cc21ed.html","excerpt":"","text":"Kong（https://github.com/Kong/kong）是一个云原生，高效，可扩展的分布式API 网关。 自 2015 年在 github 开源后，广泛受到关注，目前已收获 1.68w+ 的 star，其核心价值在于高性能和可扩展性。 为什么需要 API 网关 img 在微服务架构之下，服务被拆的非常零散，降低了耦合度的同时也给服务的统一管理增加了难度。如上图左所示，在旧的服务治理体系之下，鉴权，限流，日志，监控等通用功能需要在每个服务中单独实现，这使得系统维护者没有一个全局的视图来统一管理这些功能。API 网关致力于解决的问题便是为微服务纳管这些通用的功能，在此基础上提高系统的可扩展性。如右图所示，微服务搭配上 API 网关，可以使得服务本身更专注于自己的领域，很好地对服务调用者和服务提供者做了隔离。 为什么是 KongSpringCloud 玩家肯定都听说过 Zuul 这个路由组件，包括 Zuul2 和 Springcloud Gateway 等框架，在国内的知名度都不低。没错，我称呼这些为组件 Or 框架，而 Kong 则更衬的上产品这个词。在此我们可以简单对比下 Zuul 和 Kong。 举例而言，如果选择使用 Zuul，当需要为应用添加限流功能，由于 Zuul 只提供了基本的路由功能，开发者需要自己研发 Zuul Filter，可能你觉得一个功能还并不麻烦，但如果在此基础上对 Zuul 提出更多的要求，很遗憾，Zuul 使用者需要自行承担这些复杂性。而对于 Kong 来说，限流功能就是一个插件，只需要简单的配置，即可开箱即用。 Kong 的插件机制是其高可扩展性的根源，Kong 可以很方便地为路由和服务提供各种插件，网关所需要的基本特性，Kong 都如数支持： 云原生 : 与平台无关，Kong可以从裸机运行到Kubernetes 动态路由 ：Kong 的背后是 OpenResty+Lua，所以从 OpenResty 继承了动态路由的特性 熔断 健康** 检查 日志 : 可以记录通过 Kong 的 HTTP，TCP，UDP 请求和响应。 鉴权 : 权限控制，IP 黑白名单，同样是 OpenResty 的特性 SSL : Setup a Specific SSL Certificate for an underlying service or API. 监控 : Kong 提供了实时监控插件 认证 : 如数支持 HMAC, JWT, Basic, OAuth2.0 等常用协议 限流 REST API : 通过 Rest API 进行配置管理，从繁琐的配置文件中解放 可用性 : 天然支持分布式 高性能 : 背靠非阻塞通信的 nginx，性能自不用说 插件机制 : 提供众多开箱即用的插件，且有易于扩展的自定义插件接口，用户可以使用 Lua 自行开发插件 上面这些特性中，反复提及了 Kong 背后的 OpenResty，实际上，使用 Kong 之后，Nginx 可以完全摒弃，Kong 的功能是 Nginx 的父集。 而 Zuul 除了基础的路由特性以及其本身和 SpringCloud 结合较为紧密之外，并无任何优势。 Kong 的架构 image-20180712184740981 从技术的角度讲，Kong 可以认为是一个 OpenResty 应用程序。 OpenResty 运行在 Nginx 之上，使用 Lua 扩展了 Nginx。 Lua 是一种非常容易使用的脚本语言，可以让你在 Nginx 中编写一些逻辑操作。之前我们提到过一个概念 Kong = OpenResty + Nginx + Lua，但想要从全局视角了解 Kong 的工作原理，还是直接看源码比较直接。我们定位到本地的 Kong 文件夹，按照上图中的目录层级来识识 Kong 的庐山真面目。 Kong 文件下包含了全部源码和必要组件，分析他们，我们便得到了 Kong 的架构。13.x 是目前 Kong 的最新版本。 从 2 号块中可以看到 nginx.conf ，这其实便是一个标准的 Nginx 目录结构，这也揭示了 Kong 其实就是运行在 Nginx 的基础之上，而进行的二次封装。由 share 文件夹向下展开下一次分析。 share 文件夹中包含了 OpenResty 的相关内容，其实背后就是一堆 Lua 脚本，例如 lapis 包含了数据库操作，Nginx 生命周期，缓存控制等必要的 Lua 脚本，logging 包含了日志相关的 Lua 脚本，resty 包含了 dns，健康检查等相关功能的 Lua 脚本…而其中的 kong 目录值得我们重点分析，他包含了 Kong 的核心对象。 api 和 core 文件夹，封装了 Kong 对 service，route，upstream，target 等核心对象的操作代码（这四个核心对象将会在下面的小节重点介绍），而 plugins 文件夹则是 Kong 高可扩展性的根源，存放了 kong 的诸多扩展功能。 plugins 文件夹包含了上一节提到的 Kong 的诸多插件功能，如权限控制插件，跨域插件，jwt 插件，auth2 插件…如果需要自定义插件，则需要将代码置于此处。 从上述文件夹浏览下来，大概可以看到它和 Nginx 的相似之处，并在此基础之上借助于 Lua 对自身的功能进行了拓展，除了 nginx.conf 中的配置，和相对固定的文件层级，Kong 还需要连接一个数据库来管理路由配置，服务配置，upstream 配置等信息，是的，由于 Kong 支持动态路由的特性，所以几乎所有动态的配置都不是配置在文件中，而是借助于 Postgres 或者 Cassandra 进行管理。 postgres Kong 对外暴露了 Restful API，最终的配置便是落地在了数据库之中。 Kong 的管理方式通过文件夹结构的分析，以及数据库中的表结构，我们已经对 Kong 的整体架构有了一个基本的认识，但肯定还存在一个疑问：我会配置 Nginx 来控制路由，但这个 Kong 应当怎么配置才能达到相同的目的呢？莫急，下面来看看 Kong 如何管理配置。 Kong 简单易用的背后，便是因为其所有的操作都是基于 HTTP Restful API 来进行的。 kong端点 其中 8000/8443 分别是 Http 和 Https 的转发端口，等价于 Nginx 默认的 80 端口，而 8001 端口便是默认的管理端口，我们可以通过 HTTP Restful API 来动态管理 Kong 的配置。 一个典型的 Nginx 配置 12345678910upstream helloUpstream &#123; server localhost:3000 weight=100;&#125;server &#123; listen 80; location /hello &#123; proxy_pass http://helloUpstream; &#125;&#125; 如上这个简单的 Nginx 配置，便可以转换为如下的 Http 请求。 对应的 Kong 配置 123456789# 配置 upstreamcurl -X POST http://localhost:8001/upstreams --data \"name=helloUpstream\"# 配置 targetcurl -X POST http://localhost:8001/upstreams/hello/targets --data \"target=localhost:3000\" --data \"weight=100\"# 配置 servicecurl -X POST http://localhost:8001/services --data \"name=hello\" --data \"host=helloUpstream\"# 配置 routecurl -X POST http://localhost:8001/routes --data \"paths[]=/hello\" --data \"service.id=8695cc65-16c1-43b1-95a1-5d30d0a50409\"curl -X POST http://localhost:8001/routes --data \"hosts[]=a.com,b.com,*.abc.com\" --data \"service.id=8695cc65-16c1-43b1-95a1-5d30d0a50409\" 这一切都是动态的，无需手动 reload nginx.conf。 我们为 Kong 新增路由信息时涉及到了 upstream，target，service，route 等概念，他们便是 Kong 最最核心的四个对象。（你可能在其他 Kong 的文章中见到了 api 这个对象，在最新版本 0.13 中已经被弃用，api 已经由 service 和 route 替代） 从上面的配置以及他们的字面含义大概能够推测出他们的职责，upstream 是对上游服务器的抽象；target 代表了一个物理服务，是 ip + port 的抽象；service 是抽象层面的服务，他可以直接映射到一个物理服务(host 指向 ip + port)，也可以指向一个 upstream 来做到负载均衡；route 是路由的抽象，他负责将实际的 request 映射到 service。 他们的关系如下 upstream 和 target ：1 对 n service 和 upstream ：1 对 1 或 1 对 0 （service 也可以直接指向具体的 target，相当于不做负载均衡） service 和 route：1 对 n 高可扩展性的背后—插件机制Kong 的另一大特色便是其插件机制，这也是我认为的 Kong 最优雅的一个设计。 文章开始时我们便提到一点，微服务架构中，网关应当承担所有服务共同需要的那部分功能，这一节我们便来介绍下，Kong 如何添加 jwt 插件，限流插件。 插件（Plugins）装在哪儿？对于部分插件，可能是全局的，影响范围是整个 Kong 服务；大多数插件都是装在 service 或者 route 之上。这使得插件的影响范围非常灵活，我们可能只需要对核心接口进行限流控制，只需要对部分接口进行权限控制，这时候，对特定的 service 和 route 进行定向的配置即可。 为 hello 服务添加50次/秒的限流 123curl -X POST http://localhost:8001/services/hello/plugins \\--data \"name=rate-limiting\" \\--data \"config.second=50\" 为 hello 服务添加 jwt 插件 12curl -X POST http://localhost:8001/services/login/plugins \\--data \"name=jwt\" 同理，插件也可以安装在 route 之上 123456curl -X POST http://localhost:8001/routes/&#123;routeId&#125;/plugins \\--data \"name=rate-limiting\" \\--data \"config.second=50\"curl -X POST http://localhost:8001/routes/&#123;routeId&#125;/plugins \\--data \"name=jwt\" 在官方文档中，我们可以获取全部的插件 https://konghq.com/plugins/，部分插件需要收费的企业版才可使用。 kong插件 总结Kong 是目前市场上相对较为成熟的开源 API 网关产品，无论是性能，扩展性，还是功能特性，都决定了它是一款优秀的产品，对 OpenResty 和 Lua 感兴趣的同学，Kong 也是一个优秀的学习参考对象。基于 OpenResty，可以在现有 Kong 的基础上进行一些扩展，从而实现更复杂的特性，比如我司内部的 ABTest 插件和定制化的认证插件，开发成本都相对较低。Kong 系列的文章将会在以后持续连载。 阅读扩展 初识 Kong 之负载均衡 https://www.cnkirito.moe/kong-loadbalance/ Kong 集成 Jwt 插件 https://www.cnkirito.moe/kong-jwt/","categories":[{"name":"API网关","slug":"API网关","permalink":"https://1630425.github.io/categories/API网关/"}],"tags":[{"name":"Kong","slug":"Kong","permalink":"https://1630425.github.io/tags/Kong/"}]},{"title":"Mac使用技巧|iPic图床设置-Markdown写作必备","slug":"Mac使用技巧-iPic图床设置-Markdown写作必备","date":"2018-04-20T03:11:13.000Z","updated":"2019-06-05T03:03:41.004Z","comments":true,"path":"posts/323926de.html","link":"","permalink":"https://1630425.github.io/posts/323926de.html","excerpt":"","text":"年少时，常闲逛于1024，看美图时，时常会碰到红××或？，着实扫兴啊。楼下就常有人喊道：图床挂啦，楼主换图床啦。虽游荡网络很多年，一直不知道图床是啥东东，真是那个啥…… 前不久，跟着易仁永澄老师搭建知识主题网站，在MarkEditor插入图片同步Bitcron后分享，时常看不到图片，着实郁闷啊。只能通过简书的私密文件夹上传复制出链接才得以解决，着实麻烦，繁琐累人！ 为了便利性和储存的相对安全性，就考虑设置自己的图床了。图床是什么？ 折腾图床期间入了一些坑，下面做简单安装说明，给有需要的朋友 环境说明 系统：Mac OS 软件：iPic（注：仅苹果OS系统可用）+ 阿里云 OSS（阿里云对象存储） 参考教程： 易仁永澄：7 搞定一个图床（可选） iPic - 图床神器 在 iPic 中添加阿里云 OSS 软件安装过程比较简单就不展开了，上面教程已经很详尽了，如仍旧搞不定的，欢迎来骚扰我。 下面针对关键点：图床的类型做详细的讲解。 图床 OSS（对象存储）类型选用使用 iPic 默认免费微博图床 软件安装好后，都不用设置，直接就可以用了，但老江湖易仁永澄提醒：一定要付费，付费才靠谱！建议采用付费图床，不知道哪天微博图床说关就关了，当年的新浪问问就是前车之鉴啊。- 使用阿里云OSS（对象存储），iPic网址前缀：根据Bucket名称自动生成的域名 按照易仁永澄的教程，一步一步的操作没有问题，不掉坑的话，10分钟搞定。下图是关键设置点： ① Bucket默认网址生成，细节见下图示意 ② iPic图床设置 如不清楚Access Key 和 Sectet Key ，请看易仁永澄的指导文件或付费购买 相关教程。（当然找我也可以啦，就当交个朋友啦） 官方说明需要订阅才能使用第三方 OSS，实际我没有订阅但也能使用，不知道实际是什么情况，可能是试用期使用。（APP Store经常抽风或来姨妈的） 使用阿里云OSS（对象存储），iPic 网址前缀：使用独立域名，彰显个人品牌。被永澄老大的彰显个人品牌给吸引了，于是想搞定独立域名的设置，哪知道——这是一个坑，搞了两个小时没有搞定，最终老老实实的回到了方案⑵。 入坑记如下：⑴ 按照永澄老大的教程，进行绑定域名的设置，提交后显示不能备案 ⑵ 查找 『备案』 的相关信息 阿里云指导说明很详尽，但本人缺少网站域名相关的背景知识，这些文件说明看的我云里雾里，按照说明左点右点，愣是没有搞定独立域名。 提示重点： 实名注册 不等于 备** 案** 最终回到了：到底『什么是备案』的问题上来了，官方的说明如下： 总结如下 ： 永澄老大的域名已经过备案了，且比较早，备案也很容易，所以老大可以很方便设置二级独立域名。（按照现在的网络环境，新备案估计有点难了） 没有经过备案的独立域名是不能直接访问的 我们现申请的域名可访问的原因是: 与 Bitcron 域名捆绑了，本质上访问的是 Bitcron 的服务器。（这点理解不一定对。） ⑶ 放弃备案的原因，看图不说话 可以考虑国外域名申请，不用备案 按永澄老大教程进行图床设置的注意点iPic 一键上传快捷键 永澄老大的快捷键为：Command + Shift + R（一键上传）； iPic 默认快捷键为：Command + U如果没开iPic 的官方指导说明，你会发现图片怎么也不能一键上传。可以根据使用习惯设置自己的快捷键。 iPic 图床 - 网站前缀的设置 如果自己独立域名没有备案，在这里应该填根据Bucket名称默认生成的网址。 网站前缀不能留空 网址前缀不能留空，留空无法通过验证 插图要更换位置：四、高阶准备 - Step1：绑定自己的独立域名 如果独立域名已有备案，可以自行研究一下，按照提示操作就可以了 如果独立域名 没有 备案，请直接忽略，不用往下看了，看了也白看。 OK，iPic图床设置介绍结束，用苹果OS系统的本身比较小众，给有需要的人吧。","categories":[{"name":"图床","slug":"图床","permalink":"https://1630425.github.io/categories/图床/"}],"tags":[{"name":"iPic","slug":"iPic","permalink":"https://1630425.github.io/tags/iPic/"}]},{"title":"BaiduPCS-Go | 百度网盘命令行工具（基于 Go）","slug":"BaiduPCS-Go-百度网盘命令行工具（基于-Go）","date":"2018-02-08T04:01:18.000Z","updated":"2019-06-05T03:03:40.992Z","comments":true,"path":"posts/f44f6d37.html","link":"","permalink":"https://1630425.github.io/posts/f44f6d37.html","excerpt":"","text":"提到百度网盘，想必大家都很熟悉吧。 百度网盘自 2012 年上线运行以来，迅速积累了大量用户。但是狗改不了吃屎，作为百度的产品，百度网盘现在是越来越恶心了，不给充钱就限速，官方居然还不承认（百度网盘 - 维基百科 ）其实给免费账号限速也是可以理解的（毕竟别人也是要吃饭的，没有利润的产品肯定是走不远的）但百度居然给我限速到 20 KB/s！这还能用？！！ 但人们的力量是强大的，被百度恶心到的用户们很快就找到了破解百度网盘限速的方法—— 多线程下载 。你不是只给我 20 KB/s 吗？我开TM 500 个线程，一样把 10 M/s 的带宽占满。于是乎一大批第三方网盘拔地而起，例如：&lt;del&gt;PanDownload&lt;/del&gt;（已停止运营）、Village（Android）、油猴脚本 等。 今天介绍的 BaiduPCS-Go 也是其中一个，相比于其他第三方网盘，具有以下特点： 不需要 Aria2、IDM 等第三方软件 自定义线程数（建议将最大线程数设置为50 ~ 500，线程开太多会造成搞负载） 支持通配符匹配路径 通配符-维基百科 命(bi)令(ge)行(gao) BaiduPCS-Go 的 Github 安装得益于 Go 的跨平台编译的特性，BaiduPCS-Go 几乎可以支持所有操作系统，只需要在 Releases 中选取合适的版本下载、运行就可以了 下载BaiduPCS-Go 以 Windows 为例，根据我的电脑 CPU 下载并解压 BaiduPCS-Go-v3.2.1-windows-x86.zip 可以直接双击运行（进入 console 模式） 也可以在命令行中运行 在Android与iOS上安装在 Android 上安装与在桌面上安装的思路差不多，都是 下载 -\\&gt; 解压 -\\&gt; (在命令行中)运行 不同的是 Android 上没有原生的命令行，需要借助 Termux 或 NeoTerm 或 终端模拟器等 APP，以提供终端环境。 详情请参考：Android 运行本 BaiduPCS-Go 程序参考示例 iOS 就比较特殊了，因为 iOS 系统的特殊性，需要越狱后才能提供相应的运行环境。 越狱后,，在 Cydia 搜索下载并安装 MobileTerminal，以提供终端环境。 命令列表在命令行中，在 BaiduPCS-Go.exe 所在的目录下，使用以下格式输入命令1BaiduPCS-Go [global options] command [command options] [arguments...] 简单的说就是以 BaiduPCS-Go 开头，后面跟具体的命令（参数）。 未带任何其他参数运行程序,，则程序进入 console 模式。（光标前有 BaiduPCS-Go \\&gt; 的前缀）console 模式下直接输入命令，不需要加 BaiduPCS-Go 的前缀。 登录既然是第三方百度网盘，肯定要登录百度账号才能使用。 有两种方法可以登录， 常规登录 和 BDUSS 登录 常规登录直接键入以下命令1BaiduPCS-Go login 然后依次输入 用户名 和 密码 登录成功！ BDUSS登录获取百度 BDUSS 获得 BDUSS 后，用以下命令登录（[BDUSS] = 你取得的 BDUSS）1BaiduPCS-Go login -bduss=[BDUSS] 获取当前账号&amp;已有的账号1BaiduPCS-Go loglist 切换已登录的百度帐号1BaiduPCS-Go su -uid=[uid] 12BaiduPCS-Go su请输入要切换帐号的 index 值 \\&amp;gt;[index 值] 退出已登录的百度帐号1BaiduPCS-Go logout -uid=[uid] 12BaiduPCS-Go logout请输入要退出帐号的 index 值 &gt; [index 值] 因为我只有一个百度账号，就不演示第二种退出方式了 获取配额（获取网盘总空间和已使用空间）1BaiduPCS-Go quota 文件/目录操作对 目录、文件的操作与 Linux 命令行相似。 目录、文件名可以使用通配符（*） 切换工作目录1BaiduPCS-Go cd [目录] 输出当前所在目录1BaiduPCS-Go pwd 列出当前工作目录的文件和目录或指定目录1BaiduPCS-Go ls 1BaiduPCS-Go ls [目录] 获取单个文件/目录的元信息(详细信息)1BaiduPCS-Go meta [文件/目录] 如果没有指定的目录则默认为 获取根目录的元信息 创建目录1BaiduPCS-Go mkdir [目录] 删除文件/目录1BaiduPCS-Go rm [文件或目录1] [文件或目录2] [文件或目录3] ... 复制文件/目录12BaiduPCS-Go cp [文件/目录] [目标 文件/目录]BaiduPCS-Go cp [文件/目录1] [文件/目录2] [文件/目录3] ... [目标目录] 复制文件/目录时，需确保每个文件/目录的有效性 移动/重命名文件/目录复制文件/目录1234移动: BaiduPCS-Go mv &lt;文件/目录1&gt; &lt;文件/目录2&gt; &lt;文件/目录3&gt; ... &lt;目标目录&gt;重命名: BaiduPCS-Go mv &lt;文件/目录&gt; &lt;重命名的文件/目录&gt; 下载文件12BaiduPCS-Go download [文件或目录1] [文件或目录2] [文件或目录3]...BaiduPCS-Go d [文件或目录1] [文件或目录2] [文件或目录3]... 支持同时下载多个文件/目录 下载的文件默认保存到程序所在目录的download目录下，你可以自定义储存目录 上传文件12BaiduPCS-Go upload [本地文件或目录1] [文件或目录2] ... [网盘的目标目录]BaiduPCS-Go u [本地文件或目录1] [文件或目录2] ... [网盘的目标目录] 注意： 本地的目录要使用\\转义（两个反斜杠\\） 区别反斜杠 \\ 和 斜杠 / 例如：12345678910将本地的 C:\\Users\\Administrator\\Desktop\\1.mp4 上传到网盘 /视频 目录本地的目录要使用 &amp;quot;\\&amp;quot; 转义（两个反斜杠 &amp;quot;\\&amp;quot;）注意区别反斜杠 &amp;quot;\\&amp;quot; 和 斜杠 &amp;quot;/&amp;quot;BaiduPCS-Go upload C:\\\\Users\\\\Administrator\\\\Desktop\\\\1.mp4 /视频将本地的 C:\\Users\\Administrator\\Desktop\\1.mp4 和 C:\\Users\\Administrator\\Desktop\\2.mp4 上传到网盘 /视频 目录BaiduPCS-Go upload C:\\\\Users\\\\Administrator\\\\Desktop\\\\1.mp4 C:\\\\Users\\\\Administrator\\\\Desktop\\\\2.mp4 /视频将本地的 C:\\Users\\Administrator\\Desktop 整个目录上传到网盘 /视频 目录BaiduPCS-Go upload C:\\\\Users\\\\Administrator\\\\Desktop /视频 设置在 BaiduPCS-Go 中，使用以下格式的命令进行设置1BaiduPCS-Go set OptionName Value 翻译一下就是1BaiduPCS-Go set [被设置的项目] [你设置的值] 让我们先来看一下有哪些项目可以被设置1BaiduPCS-Go set -h 简单介绍一下 appid BaiduPCS-Go 的应用ID，一般没必要改 user_agent浏览器标识，用来伪装成正版&quot;百度云管家&quot;的（如果能下载且速度不慢就没必要改） cache_size下载缓存大小，一般没必要改 max_parallel最大线程数 -\\&gt; 设置最大线程数 savedir下载文件的储存目录 -\\&gt; 设置自定义储存目录 自定义储存目录下载文件默认保存在程序所在目录的download目录下，使用以下命令自定义储存目录1BaiduPCS-Go set savedir [储存目录的路径] 例如：12# 设置保存目录, 保存到 D:\\Downloads (注意两个反斜杠 &amp;quot;\\&amp;quot; )BaiduPCS-Go set savedir D:\\\\Downloads 设置最大线程数理论上（在没有占满带宽的情况下），线程开得越多下得越快，同时占用资源越多； 建议开到50 ~ 500（下载速度不仅仅取决于线程数，也取决于带宽大小；如果增加带宽却没有提速，说明瓶颈在带宽上）；如果觉得下载文件时电脑运行卡顿，就开小一点。 设置方法 BaiduPCS-Go set max_parallel [最大并发数] 例如： 设置下载最大并发数为 150BaiduPCS-Go set max_parallel 150 退出程序运行命令 quit 或 exit 或 组合键 Ctrl+C 或 组合键 Ctrl+D 已知问题 下载进度到最后的时候,，下载速度会降低。 程序的 console 模式在 windows 下部分中文无法正常输入。 参考本文章参考了： GitHub - iikira/BaiduPCS-Go:百度网盘工具箱- Go语言编写 使用第三方百度网盘是有风险的，如果你因为使用第三方百度网盘导致账号被封，本博客概不负责","categories":[],"tags":[]},{"title":"MarkDown添加图片的三种方式","slug":"MarkDown添加图片的三种方式","date":"2018-02-07T16:09:00.000Z","updated":"2019-06-05T03:03:41.004Z","comments":true,"path":"posts/2ef33b20.html","link":"","permalink":"https://1630425.github.io/posts/2ef33b20.html","excerpt":"","text":"插图最基础的格式就是：![Alt text](图片链接 “optional title”) Alt text：图片的Alt标签，用来描述图片的关键词，可以不写。最初的本意是当图片因为某种原因不能被显示时而出现的替代文字，后来又被用于SEO，可以方便搜索引擎根据Alt text里面的关键词搜索到图片。 图片链接：可以是图片的本地地址或者是网址。”optional title”：鼠标悬置于图片上会出现的标题文字，可以不写。 插入本地图片只需要在基础语法的括号中填入图片的位置路径即可，支持绝对路径和相对路径。例如： ![avatar](/home/picture/1.png) 不灵活不好分享，本地图片的路径更改或丢失都会造成markdown文件调不出图。 插入网络图片只需要在基础语法的括号中填入图片的网络链接即可，现在已经有很多免费/收费图床和方便传图的小工具可选。例如： ![avatar](https://www.baidu.com/img/bd_logo1.png) 将图片存在网络服务器上，非常依赖网络。 把图片存入markdown文件用base64转码工具把图片转成一段字符串，然后把字符串填到基础格式中链接的那个位置。 基础用法：![avatar](data:image/png;base64,iVBORw0……)这个时候会发现插入的这一长串字符串会把整个文章分割开，非常影响编写文章时的体验。如果能够把大段的base64字符串放在文章末尾，然后在文章中通过一个id来调用，文章就不会被分割的这么乱了。 高级用法比如：![avatar][base64str][base64str]:data:image/png;base64,iVBORw0……","categories":[],"tags":[]},{"title":"Shell在文本第一行和最后一行添加字符串","slug":"shell在文本第一行和最后一行添加字符串","date":"2017-12-16T05:04:54.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/17657829.html","link":"","permalink":"https://1630425.github.io/posts/17657829.html","excerpt":"","text":"1234sed -i '1i 添加的内容' file #这是在第一行前添加字符串sed -i '$i 添加的内容' file #这是在最后一行行前添加字符串sed -i '$a添加的内容' file #这是在最后一行行后添加字符串echo \"添加的内容\" &gt;&gt; file #这是在最后一行行后添加字符串","categories":[{"name":"Linux","slug":"Linux","permalink":"https://1630425.github.io/categories/Linux/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://1630425.github.io/tags/Shell/"}]},{"title":"iPic-Markdown Mac图床、文件上传工具","slug":"iPic-Markdown-Mac图床、文件上传工具","date":"2017-11-28T02:26:15.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/6ca1c9fd.html","link":"","permalink":"https://1630425.github.io/posts/6ca1c9fd.html","excerpt":"","text":"有了图床神器 iPic，不论屏幕截图、还是复制图片，都可以自动上传、保存 Markdown 格式的链接，直接粘贴插入，够懒人吧？ 使用 Hexo | Heroku 或 WordPress 写博客、在公众号发文章、在知乎讨论、在豆瓣灌水、在论坛发帖、跨境做外贸电商 … iPic 带给你从未有过的插图体验。 当然，除了图片，你可以 上传普通文件 ，上传方式与图片完全相同。 上传方式图床工具 iPic 支持多种图片上传方式。下面我们来简单看下各个上传方式、以及分别适合在什么场景下使用。 1.拖拽图片上传拖动是比较好玩的一种上传方式。只要将图片拖到菜单栏的 iPic 图标上，松手后就可以自动上传。 可以注意到，上传时菜单栏图标也会显示上传的进度。很简洁、却很实用，不再盲目等待。 使用这种方式，还可以一次性上传多张图片。图片上传后的顺序，和上传前选择的顺序一致。 2.使用服务上传图片在 Finder 中使用 服务 上传也是很高效的方式。只要在图片上右击、然后选择服务中的 使用 iPic 上传 即可。 除了使用菜单，更高效的方式是使用快捷键。只要选中图片，然后按下 Command + U 快捷键，即可自动上传。 如果你觉得默认快捷键 Command + U 不方便，也可以在 系统偏好设置 \\&gt; 键盘 \\&gt; 快捷键 \\&gt; 服务中修改 使用 iPic 上传 对应的快捷键。同样，如果你的 Mac 中安装了很多程序、菜单中有很多你不需要的服务，也可以在这里进行关闭。 使用服务上传还有其他便利之处： 可以一次性上传多张图片 即使 iPic 并未运行，系统也会启动 iPic、并自动上传 注意：由于 macOS 系统更新机制的缘故，新安装 iPic 后上传服务可能未出现、或未翻译，可以等几分钟、甚至几小时后再试，iPic 上传服务就会正常显示；也可以在 终端 手动更新服务菜单： /System/Library/CoreServices/pbs -update 3.复制图片后上传iPic 会自动监测剪切板的变化，当复制图片后，该图片会出现在 iPic 菜单中 待上传 区域。如果需要上传，点击菜单中该图片即可。手动上传比较适合临时上传少量图片。 除了手动点击菜单，还可以使用快捷键 Command + Shift + U 上传。当然，可以在偏好设置中修改此快捷键。 4.上传其他App中的图片上述示例中主要介绍了图片文件的上传。另外，iPic 还支持支持其他程序中图片的上传。例如： 其中，对于图片格式，常见的 jpg、png、gif 等格式都是支持。 上传图片相关设置上传前压缩图片可以在 iPic 的 偏好设置 中开启「上传前压缩图片」选项，目前支持压缩的图片格式：jpg、png、gif 采用有损压缩，压缩后肉眼几乎无法看出区别，却可明显降低图片尺寸。使用压缩后的图片，既可以节省图片的存储空间，还可以加快图片加载速度、节省流量。 上传后不播放声音iPic 上传后会使用系统通知来提示。如果不喜欢该通知的声音，可进入 系统偏好设置 \\&gt; 通知，在左侧列表选择 iPic，然后在右侧取消「播放通知的声音」。 图床图床也即你选择存放图片的云服务。可以在 iPic 的 偏好设置 中添加你的图床： 添加后，可以在 iPic 的菜单中选择当前使用的图床： 目前 iPic 支持下列图床：微博图床（即默认图床）、七牛云 、又拍云、阿里云 OSS 、腾讯云 COS、Imgur 、Flickr 、Amazon S3 iPic菜单Markdown链接 这里有个很贴心的功能：切换普通链接、Markdown 格式链接时，如果粘贴板中有上一格式的内容，会转换后重新保存到粘贴板中。 图片上传记录iPic 会保存最近上传的 15 张图片，其中最后上传的 3 张图片会出现在一级菜单中，其他的则在 更多已上传图片 中。 点击已上传图片，则会复制该图片的链接。 当然，可以在 更多 菜单中清空图片上传记录。 iPic MoveriPic Mover 可以一键将已有 Markdown 文件中所有图片迁移至新图床。批量上传图片、图床搬家，从未如此简单。 下载 iPic 下载配合默认图床，可免费使用所有功能。如需使用其他图床，订阅 iPic 高级版即可。暂不支持 Windows.","categories":[{"name":"图床","slug":"图床","permalink":"https://1630425.github.io/categories/图床/"}],"tags":[{"name":"iPic","slug":"iPic","permalink":"https://1630425.github.io/tags/iPic/"}]},{"title":"基于docker-compose搭建分布式消息队列Kafka","slug":"基于docker-compose搭建分布式消息队列Kafka","date":"2017-11-24T14:33:34.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/7fb66013.html","link":"","permalink":"https://1630425.github.io/posts/7fb66013.html","excerpt":"","text":"本文基于Docker Compose搭建一套单节点的Kafka消息队列，Kafka依赖Zookeeper为其管理集群信息，虽然本例不涉及集群，但是该有的组件都还是会有，典型的kafka分布式架构如下图所示。本例搭建的示例包含Zookeeper + Kafka + Kafka-manger Kafka官网： http://kafka.apache.org/ 重要概念生产者(Producer)消费者(Consumer)消费消息。每个consumer属于一个特定的consumer group。使用consumer high level API时，同一个topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。 集群(Cluster)宏观来看，Kafka主体包含的就是三部分: 生产者、消费者和集群，一个集群就是多个Broker的集合。 Broker已经发布的消息就会保存在集群中的某个Broker中去。 Topic用来区别message的种类，比如很多时候，与A相关的日志统一的topic定义为A，B相关的日志统一的topic定义为B，这样就不用一个一个单独地订阅了。物理上不通topic的消息分开存储，逻辑上一个topic的消息虽然保存于一个或多个broker上，但是用户只需指定消息的topic即可生产或消费数据而不必关心数据在哪里。 PartitionKafka中每个Topic都会有一个或多个Partition，他是Kafaka数据存储的基本单元，每个Partition对应一个文件夹，文件夹下存储这个Partition的所有消息和索引。Kafka内部会根据算法得出一个值，根据这个值放入对应的partition目录中。所以读取时间复杂度为O(1)。分区的每一个消息都有一个连续的序列号叫做offset，用来在分区中唯一标识这个消息。一个topic可以保存在多个partition。 Segment一个partition由多个Segment组成，一个Partition代表一个文件夹，一个Segment则代表该文件夹下的文件。Segment有大小限制，由log.segment.bytes 定义。 安装Docker Compose方式安装docker-compose.yml文件:123456789101112131415161718192021222324252627282930313233version: '2'services: zookeeper: image: wurstmeister/zookeeper environment: JMX: 9000 ports: - \"2181:2181\" kafka: image: wurstmeister/kafka #这个镜像使用文档见https://github.com/wurstmeister/kafka-docker ports: - \"9092:9092\" expose: - \"9092\" environment: KAFKA_ADVERTISED_HOST_NAME: 10.19.131.157 #docker宿主机的IP，直接ifconfig获取，这是重点，否则，在容器内部启动生产者消费者都会失败的 KAFKA_CREATE_TOPICS: \"test:1:1\" #自动创建一个默认的topic KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"false\" #禁用掉自动创建topic的功能，使用上面的镜像，kafka的参数设置都可以以这样的方式进行设置 volumes: - /var/run/docker.sock:/var/run/docker.sock kafka-manager: image: sheepkiller/kafka-manager #如果要安装web管理工具可以同时安装这个，最后通过宿主机IP的9000端口进行访问，例如http://127.0.0.1:9000/ links: - kafka - zookeeper environment: ZK_HOSTS: zookeeper:2181 APPLICATION_SECRET: \"letmein\" ports: - \"9000:9000\" expose: - \"9000\" 安装命令:12345678910111213#默认只会有一个kafka实例docker-compose up -d/* 运行kafka集群模式 *//* 由于指定了kafka对外暴露的端口号，增加集群节点会报端口冲突的错误，请将kafka暴露的端口号删掉后再执行如下命令 动态端口：将- \"9092:9092\"更改为- \"9092\"，去掉expose:- \"9092\"，这种会生成动态端口；非动态端口，就是自己指定端口和对应实例个数即可 *//* 自己指定kafka的节点数量 */#将kafka实例增加到n个，推荐使用3个，就能直接建立一个集群docker-compose scale kafka=3# 暂停所有容器docker-compose stop# 开启所有容器docker-compose start# 删除所有容器docker-compose rm -f webKafka Managerhttp://127.0.0.1:9000/ Cluster——》addCluster——》Cluster Name——》my-kafka Cluster Zookeeper Hosts——》zookeeper:2181其他默认即可，Save kafka命令1docker exec -it data_kafka_1 bash kafka-console-consumer.sh12#启动一个消费者，监听test这个topickafka-console-consumer.sh --bootstrap-server 10.19.131.157:9092 --from-beginning --topic test kafka-console-producer.sh12#启动一个生产者，直接输入消息回车即可发送消息了kafka-console-producer.sh --broker-list 10.19.131.157:9092 --topic test kafka-consumer-groups.sh1234#查看新消费者列表kafka-consumer-groups.sh --new-consumer --bootstrap-server 10.19.131.157:9092 --list#查看某消费者的消费详情，这里的消费者名称就是kafka-python-default-groupkafka-consumer-groups.sh --new-consumer --bootstrap-server 10.19.131.157:9092 --describe --group kafka-python-default-group kafka-producer-perf-test.sh自带的压测工具12#总共100条数据，每条大小是1kafka-producer-perf-test.sh --topic test --num-records 10000 --record-size 1 --throughput 100 --producer-props bootstrap.servers=10.19.131.157:9092 kafka-topics.sh1234#列出所有的topickafka-topics.sh --list --zookeeper zookeeper:2181#查看集群描述kafka-topics.sh --describe --zookeeper zookeeper:2181 安全认证Kafka可以配合SSL+ACL来进行安全认证: http://orchome.com/185 TroubleShooting容器内部启动生产者出现错误:log[2016-12-26 03:03:39,983] WARN Error while fetching metadata with correlation id 0 : {test=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient) 是因为docker-compose文件里面的宿主讥IP设置出错，如果是动态IP的话就没办法了，只能删除重新创建了","categories":[{"name":"消息队列","slug":"消息队列","permalink":"https://1630425.github.io/categories/消息队列/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://1630425.github.io/tags/Docker/"},{"name":"Kafka","slug":"Kafka","permalink":"https://1630425.github.io/tags/Kafka/"},{"name":"Docker Compose","slug":"Docker-Compose","permalink":"https://1630425.github.io/tags/Docker-Compose/"}]},{"title":"基于Docker搭建分布式消息队列Kafka","slug":"基于Docker搭建分布式消息队列Kafka","date":"2017-11-23T14:33:34.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/4d1930e3.html","link":"","permalink":"https://1630425.github.io/posts/4d1930e3.html","excerpt":"","text":"本文基于Docker搭建一套单节点的Kafka消息队列，Kafka依赖Zookeeper为其管理集群信息，虽然本例不涉及集群，但是该有的组件都还是会有，典型的kafka分布式架构如下图所示。本例搭建的示例包含Zookeeper + Kafka + Kafka-manger Kafka官网： http://kafka.apache.org/ 重要概念 生产者(Producer) 消费者(Consumer) 消费消息。每个consumer属于一个特定的consumer group。使用consumer high level API时，同一个topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。 集群(Cluster) 宏观来看，Kafka主体包含的就是三部分: 生产者、消费者和集群，一个集群就是多个Broker的集合。 Broker 已经发布的消息就会保存在集群中的某个Broker中去。 Topic 用来区别message的种类，比如很多时候，与A相关的日志统一的topic定义为A，B相关的日志统一的topic定义为B，这样就不用一个一个单独地订阅了。物理上不通topic的消息分开存储，逻辑上一个topic的消息虽然保存于一个或多个broker上，但是用户只需指定消息的topic即可生产或消费数据而不必关心数据在哪里。 Partition Kafka中每个Topic都会有一个或多个Partition，他是Kafaka数据存储的基本单元，每个Partition对应一个文件夹，文件夹下存储这个Partition的所有消息和索引。Kafka内部会根据算法得出一个值，根据这个值放入对应的partition目录中。所以读取时间复杂度为O(1)。分区的每一个消息都有一个连续的序列号叫做offset，用来在分区中唯一标识这个消息。一个topic可以保存在多个partition。 Segment 一个partition由多个Segment组成，一个Partition代表一个文件夹，一个Segment则代表该文件夹下的文件。Segment有大小限制，由log.segment.bytes 定义。 #获取镜像 zookeeper镜像：zookeeper:3.4.9 kafka镜像：wurstmeister/kafka:0.10.2.0 kafka-manager镜像：kafka-manager:latest 建立Zookeeper容器这里我们用最简单的方式创建一个独立的Zookeeper节点，如果要考虑zookeeper的高可用，可以将其做成一个集群，最好是能有多台机器。1234docker run --name some-zookeeper \\--restart always \\-p 2181:2181 \\-d zookeeper 默认的，容器内配置文件在，/conf/zoo.cfg，数据和日志目录默认在/data 和 /datalog，需要的话可以将上述目录映射到宿主机的可靠文件目录下。详情参考Zookeeper官方镜像 建立kafka节点这里同样只做一个简单的单点kafka123456docker run --name kafka \\-p 9092:9092 \\-e KAFKA_ADVERTISED_HOST_NAME=kafka01 \\-e KAFKA_CREATE_TOPICS=\"test:1:1\" \\-e KAFKA_ZOOKEEPER_CONNECT=10.19.131.157:2181 \\-d wurstmeister/kafka 详情参考Kafka官方镜像 创建Kafka管理节点kafka-manager有图形化UI，可以方便的监控集群状态，调整队列配置123456docker run -itd \\--restart=always \\--name=kafka-manager \\-p 9000:9000 \\-e ZK_HOSTS=\"10.19.131.157:2181\" \\sheepkiller/kafka-manager 容器启动以后访问主机的9000端口，http://127.0.0.1:9000 读写验证读写验证的方法有很多，这里我们用kafka容器自带的工具来验证，首先进入到kafka容器的交互模式：1docker exec -it kafka /bin/bash 创建一个主题： 1/opt/kafka/bin/kafka-topics.sh --create --zookeeper 10.19.131.157:2181 --replication-factor 1 --partitions 1 --topic my-test 查看刚创建的主题： 1/opt/kafka/bin/kafka-topics.sh --list --zookeeper 10.19.131.157:2181 发送消息： 1/opt/kafka/bin/kafka-console-producer.sh --broker-list 10.19.131.157:9092 --topic my-test 读取消息： 1/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server 10.19.131.157:9092 --topic my-test --from-beginning 参考：https://kafka.apache.org/quickstart","categories":[{"name":"消息队列","slug":"消息队列","permalink":"https://1630425.github.io/categories/消息队列/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://1630425.github.io/tags/Docker/"},{"name":"Kafka","slug":"Kafka","permalink":"https://1630425.github.io/tags/Kafka/"}]},{"title":"支付宝红包码","slug":"支付宝红包码","date":"2017-10-31T16:12:31.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/6a3c93e4.html","link":"","permalink":"https://1630425.github.io/posts/6a3c93e4.html","excerpt":"","text":"发红包、领红包、支付宝红包码","categories":[],"tags":[]},{"title":"WebP在减少图片体积和流量上的效果如何？MIP技术实践分享","slug":"WebP在减少图片体积和流量上的效果如何？MIP技术实践分享","date":"2017-10-23T11:30:50.000Z","updated":"2019-06-05T03:03:41.008Z","comments":true,"path":"posts/7109808e.html","link":"","permalink":"https://1630425.github.io/posts/7109808e.html","excerpt":"","text":"作者 | Jackson编辑 | 尾尾 不论是 PC 还是移动端，图片一直占据着页面流量的大头，在图片的大小和质量之间如何权衡，成为了长期困扰开发者们的问题。而 WebP 技术的出现，为解决该问题提供了好的方案。本文将为大家详细介绍 WebP 技术，同时也会分享该技术在 MIP 项目中的实践。 什么是WebP？WebP 是由 Google 收购 On2 Technologies 后发展出来的图片格式，以 BSD 授权条款发布。目前已经在不同厂商之间进行了尝试，如 Google、Facebook、ebay、百度、腾讯、淘宝等。 为什么选择WebP？WebP的优势WebP 在支持有损、无损、透明图片压缩的同时，大大减少了图片的体积。据统计，WebP 无损压缩后比 PNG 图片体积减少了 26%，有损图片比同类 JPEG 图像体积减少了 25%~34%，下面总结 WebP 在不同指标上所能获得的提升对比。 体积和流量方面根据业界给出的改造数据可知，改造 WebP 之后图片体积会降低很多，具体可参照 WebP 体积测试，同时也可参照下图。 在 MIP 项目中，通过我们的本地测试获得的数据如下图所示。 从以上测试可知，如果将体积换算成带宽，WebP 不同模式下都会节省大量流量。科技博客 Gig‍‍‍aOM 曾报道，谷歌的 Chrome 网上应用商店采用 WebP 格式图片后，每天可以节省几 TB 的带宽；Google+ 移动应用采用 WebP 图片格式后，每天节省了 50TB 数据存储空间。 速度方面图片的加载速度还要取决于网络时间，所以我们没有测试确定的数据，不过可以参考业界的数据：科技博客 Gig‍‍‍aOM 曾报道，YouTube 的视频略缩图采用 WebP 格式后，网页加载速度提升了 10%；谷歌的 Chrome 网上应用商店采用 WebP 格式图片后，页面平均加载时间大约减少 1/3。 兼容性目前来说，WebP 的支持程度也在不断上升，据 2017 年 10 月 12 日在 can i use 上的查询显示，全球 WebP 的支持程度已经达到 73.64%，如下图所示。 而也正是因为这种天然的图片体积优势和发展趋势，MIP 团队也决定进行初步的实践尝试，以提升页面用户体验。 WebP实践经验如何判断浏览器支持程度？WebP 的判断方法在 官方文档 中进行了总结，大致分为 HTML5 picture、嗅探和 Request Header 三种方式，下面展开介绍这三种方式。 HTML5 picture这种方法不进行 WebP 支持程度的判断，而是利用 html5 picture 元素的特性，允许开发者列举出多个图片地址，浏览器根据顺序展示出第一个能够展现的图片元素，如 1234&lt;picture&gt; &lt;source type=\"image/webp\" srcset=\"images/webp.webp\"&gt; &lt;img src=\"images/webp.jpg\" alt=\"webp image\"&gt;&lt;/picture&gt; 上面的示例在浏览器不支持 WebP 图片的情况下自动回退到 jpg 格式进行展示，但 picture 的支持程度还不是很完善，开发者可以根据需求决定是否进行使用。 嗅探嗅探的方式是指直接向浏览器中插入一段 base64 的 WebP 图片，检测图片是否能够正常加载，依据该方法进而判断支持程度，如官方给出的嗅探函数： 1234567891011121314151617181920// check_webp_feature:// 'feature' can be one of 'lossy', 'lossless', 'alpha' or 'animation'.// 'callback(feature, result)' will be passed back the detection result (in an asynchronous way!)function check_webp_feature(feature, callback) &#123; var kTestImages = &#123; lossy: \"UklGRiIAAABXRUJQVlA4IBYAAAAwAQCdASoBAAEADsD+JaQAA3AAAAAA\", lossless: \"UklGRhoAAABXRUJQVlA4TA0AAAAvAAAAEAcQERGIiP4HAA==\", alpha: \"UklGRkoAAABXRUJQVlA4WAoAAAAQAAAAAAAAAAAAQUxQSAwAAAARBxAR/Q9ERP8DAABWUDggGAAAABQBAJ0BKgEAAQAAAP4AAA3AAP7mtQAAAA==\", animation: \"UklGRlIAAABXRUJQVlA4WAoAAAASAAAAAAAAAAAAQU5JTQYAAAD/////AABBTk1GJgAAAAAAAAAAAAAAAAAAAGQAAABWUDhMDQAAAC8AAAAQBxAREYiI/gcA\" &#125;; var img = new Image(); img.onload = function () &#123; var result = (img.width &gt; 0) &amp;&amp; (img.height &gt; 0); callback(feature, result); &#125;; img.onerror = function () &#123; callback(feature, false); &#125;; img.src = \"data:image/webp;base64,\" + kTestImages[feature];&#125; 其中包含了对有损、无损、透明图、动图等 WebP 图片的嗅探，这是一种最为保险的方法。不过缺点也很明显，在图片类型不一且量级较大的情况下，前端并不能知道哪些图片是有损，无损，亦或是透明的，也没有办法对其中一种特定类型做特定策略，所以即使知道不支持该类型的 WebP，然而我们也没有办法主观的去做容错。所以 ** 这种方法只适合于图片类型单一 ** 的情况，如开发者知道所有图片都是有损的，或是动图等，有针对性的去处理。 同时在处理的过程中，为了提高嗅探效率，嗅探之后可以将结果以本地存储的方式进行保存，如 cookie ，方便下次直接进行调用。 Request Header这种方式是较为符合标准的解决方案，浏览器在支持 WebP 图片格式的情况下，会在请求的 http header accept 中携带 webp/image 的字段，后端接收到请求之后可以按照该形式来判断是否返回 WebP 图片内容。 MIP 在实践中采用的是该方法，当用户访问 MIP Cache 上的页面时，不需要开发者替换图片，MIP Cache 根据 http header 自动决定是否返回 WebP 图片内容。 不过这个过程也依然有坑——国内浏览器层出不群，大部分都向标准化的方向靠近，但仍然需要一定的时间来跟进。所以，在实践过程中我们就发现了这样的问题：虽然 http header accept 中包含了 webp/image 的字段，但实际上是不支持 WebP 格式的（华为 MT7 自带浏览器），具体体现在动图（animation）的 feature 上。而相应的解决方案其实也很简单，就是在 WebP 图片加载失败后发起原图请求，让图片能够正确的展示在页面上，具体方式是通过 img onerror 函数执行对应逻辑。 WebP转换工具WebP 的转换工具很多，主要包含了命令行和可视化界面两种： 命令行官方对于每一种 WebP 格式也分别提供了对应的 转换工具，主要包含了 cwebp、dwebp、vwebp、webpmux、gif2webp 等几种，开发者可以择优选择。 可视化页面也提供了不同可视化的软件进行 WebP 格式图片转换，如：iSparta。 总结WebP 作为一种新型图片格式，不但能够节省流量，减少图片体积，一定程度上也可以优化用户体验。MIP 项目对于 WebP 支持目前已上线，大家可以浏览 MIP 页面进行体验。同时作为关注速度优化的 MIP 团队，我们将不断迭代前行，致力于打造极致的用户体验。","categories":[{"name":"图片格式","slug":"图片格式","permalink":"https://1630425.github.io/categories/图片格式/"}],"tags":[{"name":"WebP","slug":"WebP","permalink":"https://1630425.github.io/tags/WebP/"}]},{"title":"解除360doc网页防复制","slug":"解除360doc网页防复制","date":"2016-10-14T07:32:48.000Z","updated":"2019-06-05T03:03:41.056Z","comments":true,"path":"posts/2fc0e0a3.html","link":"","permalink":"https://1630425.github.io/posts/2fc0e0a3.html","excerpt":"","text":"方法1chrome浏览器按F12（打开谷歌浏览器的开发者工具） 然后按F5 刷新下网页，（根据测试必须刷新下网页等下插入代码才能生效）下面空白插入代码：1document.oncontextmenu=document.onselectstart=document.body.onselectstart=document.oncopy=document.body.oncopy=&quot;&quot; 然后按回车键，ok！不会弹出那令人蛋疼的提示了，可以复制了。 方法2先打开一个Word文件，然后直接选中要复制的内容，可以把复制的内容直接拖动到Word中就OK了。 发现这种方法也适合其他被保护内容的复制。 方法3把下面的代码保存成浏览器书签，遇到右键不允许复制的网站，点一下这个书签就可以去除保护，随意复制。1javascript:(function() &#123; function R(a)&#123;ona =&quot;on&quot;+a; if(window.addEventListener) window.addEventListener(a, function (e) &#123; for(var n=e.originalTarget; n; n=n.parentNode) n[ona]=null; &#125;, true); window[ona]=null; document[ona]=null; document.onkeydown=null; if(document.body) document.body[ona]=null; document.body.oncopy=null; &#125; R(&quot;contextmenu&quot;); R(&quot;click&quot;); R(&quot;mousedown&quot;); R(&quot;mouseup&quot;); R(&quot;selectstart&quot;);&#125;)() 方法4右上角菜单按钮→设置→显示高级设置→隐私设置下的 内容设置按钮→javascript下的管理例外情况→添加 [*.]360doc.com 设置为禁止 →完成 重新刷新页面即可。","categories":[],"tags":[]},{"title":"git add -A 和 git add . 的区别","slug":"git-add-A-和-git-add-的区别","date":"2016-09-10T05:30:21.000Z","updated":"2019-06-05T03:03:41.012Z","comments":true,"path":"posts/1c334725.html","link":"","permalink":"https://1630425.github.io/posts/1c334725.html","excerpt":"","text":"介绍git add -A和 git add . git add -u在功能上看似很相近，但还是存在一点差别 git add . ：他会监控工作区的状态树，使用它会把工作时的所有变化提交到暂存区，包括文件内容修改(modified)以及新文件(new)，但不包括被删除的文件。 git add -u ：他仅监控已经被add的文件（即tracked file），他会将被修改的文件提交到暂存区。add -u 不会提交新文件（untracked file）。（git add –update的缩写） git add -A ：是上面两个功能的合集（git add –all的缩写） 示例下面是具体操作例子，方便更好的理解（Git version 1.x）：12345678910111213141516171819202122232425262728293031323334353637383940414243444546git initecho Change me &gt; change-meecho Delete me &gt; delete-megit add change-me delete-megit commit -m initialecho OK &gt;&gt; change-merm delete-meecho Add me &gt; add-megit status# Changed but not updated:# modified: change-me# deleted: delete-me# Untracked files:# add-megit add .git status# Changes to be committed:# new file: add-me# modified: change-me# Changed but not updated:# deleted: delete-megit resetgit add -ugit status# Changes to be committed:# modified: change-me# deleted: delete-me# Untracked files:# add-megit resetgit add -Agit status# Changes to be committed:# new file: add-me# modified: change-me# deleted: delete-me 总结· git add -A 提交所有变化 · git add -u 提交被修改(modified)和被删除(deleted)文件，不包括新文件(new) · git add . 提交新文件(new)和被修改(modified)文件，不包括被删除(deleted)文件 git不同版本区别Git Version 1.x:Git Version 2.x:","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"https://1630425.github.io/tags/Git/"}]},{"title":"5W,5W1H,5W2H,5W2H1E","slug":"5W-5W1H-5W2H-5W2H1E","date":"2015-10-14T07:53:22.000Z","updated":"2019-06-05T03:03:40.988Z","comments":true,"path":"posts/87d9b46d.html","link":"","permalink":"https://1630425.github.io/posts/87d9b46d.html","excerpt":"","text":"5W理论&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;美国学者H·拉斯维尔于1948年在《传播在社会中的结构与功能》一篇论文中，首次提出了构成传播过程的五种基本要素，并按照一定结构顺序将它们排列，形成了后来人们称之“五W模式”或“拉斯维尔程式”的过程模式。这五个W分别是英语中五个疑问代词的第一个字母，即： Who （谁） Says What （说了什么） In Which Channal （通过什么渠道） To Whom （向谁说） With What Effect （有什么效果） 5W1H分析法也称六何分析法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5W1H分析法也称六何分析法，是一种思考方法，也可以说是一种创造技法。是对选定的项目、工序或操作，都要从原因（WHY）、对象（WHAT）、地点（WHERE）、时间（WHEN）、人员（WHO）、方法（HOW）等六个方面提出问题进行思考。这种看似很可笑、很天真的问话和思考办法，可使思考的内容深化、科学化。具体见下表： 对象（WHAT） 公司生产什么产品？车间生产什么零配件？为什么要生产这个产品？能不能生产别的？我到底应该生产什么？例如如果现在这个产品不挣钱，换个利润高 场所（WHERE） 生产是在哪里干的？为什么偏偏要在这个地方干？换个地方行不行？到底应该在什么地方干？这是选择工作场所应该考虑的。 时间和程序（WHEN） 例如现在这个工序或者零部件是在什么时候干的？为什么要在这个时候干？能不能在其他时候干？把后工序提到前面行不行？到底应该在什么时间干？ 人员（WHO） 现在这个事情是谁在干？为什么要让他干？如果他既不负责任，脾气又很大，是不是可以换个人？有时候换一个人，整个生产就有起色了。 手段（HOW） 手段也就是工艺方法，例如，现在我们是怎样干的？为什么用这种方法来干？有没有别的方法可以干？到底应该怎么干？有时候方法一改，全局就会改变。 5W2H分析法又称七何分析法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5W2H法是第二世界大战中美国陆军兵器修理部首创。简单、方便，易于理解、使用，富有启发意义，广泛用于企业管理和技术活动，对于决策和执行性的活动措施也非常有帮助，也有助于弥补考虑问题的疏漏。为什么（Why）为什么采用这个技术参数？为什么不能有响声？为什么停用？为什么变成红色：为什么要做成这个形状？为什么采用机器代替人力？为什么产品的制造要经过这么多环节？为什么非做不可？ 做什么（What）条件是什么？哪一部分工作要做？目的是什么？重点是什么？与什么有关系？功能是什么？规范是什么？工作对象是什么？ 谁（Who）谁来办最方便？谁会生产？谁可以办？谁是顾客？谁被忽略了？谁是决策人？谁会受益？ 何时（When）何时要完成？何时安装？何时销售？何时是最佳营业时间？何时工作人员容易疲劳？何时产量最高？何时完成最为时宜？需要几天才算合理？ 何地（Where）何地最适宜某物生长？何处生产最经济？从何处买？还有什么地方可以作销售点？安装在什么地方最合适？何地有资源？ 怎样（How）怎样做省力？怎样做最快？怎样做效率最高？怎样改进？怎样得到？怎样避免失败？ 怎样求发展？怎样增加销路？怎样达到效率？怎样才能使产品更加美观大方？怎样使产品用起来方便？ 多少（How much）功能指标达到多少？销售多少？成本多少？输出功率多少？效率多高？尺寸多少？重量多少？ 5W2H1E任何一种企划书的构成都必须有5W2H1E，共8个基本要素。所谓的5W2H1E即： What(什么)–企划的目的、内容。 Who( 谁)–企划相关人员。 Where( 何处)–企划实施场所。 When(何时)–企划的时间。 Why(为什么)–企划缘由、前景。 How(如何)–企划的方法和运转实施。 How much(多少)–企划预算。 Effect(效果)–预测企划结果、效果。 尤其值得一提的是，要注意How much和Effect对整个企划案的重要意义。如果忽视企划的成本投入，不注意企划书实施效果的预测，那么，这种企划就不是一种成功的企划。只有5W1H的企划书不能称之为企划书，只能算是计划书。","categories":[],"tags":[]},{"title":"test","slug":"test","date":"2013-12-24T16:00:00.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/d87f7e0c.html","link":"","permalink":"https://1630425.github.io/posts/d87f7e0c.html","excerpt":"","text":"test","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2013-12-24T15:00:00.000Z","updated":"2019-06-05T03:03:41.012Z","comments":true,"path":"posts/4a17b156.html","link":"","permalink":"https://1630425.github.io/posts/4a17b156.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"SOA、ERP、SAP和SaaS四者的区别","slug":"SOA、ERP、SAP和SaaS四者的区别","date":"2011-03-25T08:19:32.000Z","updated":"2019-06-05T03:03:41.004Z","comments":true,"path":"posts/44771869.html","link":"","permalink":"https://1630425.github.io/posts/44771869.html","excerpt":"","text":"SOA、ERP、SAP和SaaS大家熟悉吗？秘奥收集相关资料整理如下： SOA 面向服务架构SOA（Service-Oriented Architecture）是一种架构模型和一套设计方法学，其目的是最大限度地重用应用程序中立型的服务以提高IT适应性和效率。它可以根据需求通过网络对松散耦合的粗粒度应用组件进行分布式部署、组合和使用。服务层是SOA的基础，可以直接被应用调用，从而有效控制系统中与软件代理交互的人为依赖性。SOA的关键是“服务”的概念，W3C将服务定义为：“服务提供者完成一组工作，为服务使用者交付所需的最终结果。最终结果通常会使使用者的状态发生变化，但也可能使提供者的状态改变，或者双方都产生变化”。 ERPERP是英文Enterprise Resource Planning(企业资源计划)的简写。指建立在信息技术基础上，以系统化的管理思想为企业决策层及员工提供决策运行手段的管理平台。 ERP系统集中信息技术与先进的管理思想於一身，成为现代企业的运行模式，反映时代对企业合理调配资源，最大化地创造社会财富的要求，成为企业在信息时代生存、发展的基石。 SAP是一个领先的ERP软件.Systems ,Application,and Products in Data processingSAP R/3软件具备以下功能和主要特点:功能性：R/3以模块化的形式提供了一整套业务措施,其中的模块囊括了全部所需要的业务功能并把用户与技术性应用软件相联而形成一个总括的系统，用于公司或企业战略上和运用上的管理。集成化: R/3把逻辑上相关联的部分连接在一起。重复工作和多余数据被完全取消，规程被优化，集成化的业务处理取代了传统的人工操作。 SaaS SaaS是Software-as-a-service（软件即服务）的简称，它是一种通过Internet提供软件的模式，用户不用再购买软件，而改用向提供商租用基于Web的软件，来管理企业经营活动，且无需对软件进行维护，服务提供商会全权管理和维护软件，对于许多小型企业来说，SaaS是采用先进技术的最好途径，它消除了企业购买、构建和维护基础设施和应用程序的需要，近年来，SaaS的兴起已经给传统套装软件厂商带来真实的压力。秘奥软件坚持的发展战略就是：软件即是一种服务！ SaaS服务提供模式 SaaS服务提供商为中小企业搭建信息化所需要的所有网络基础设施及软件、硬件运作平台，并负责所有前期的实施、后期的维护等一系列服务，企业无需购买软硬件、建设机房、招聘IT人员，只需前期支付一次性的项目实施费和定期的软件租赁服务费，即可通过互联网享用信息系统。服务提供商通过有效的技术措施，可以保证每家企业数据的安全性和保密性。企业采用SaaS服务模式在效果上与企业自建信息系统基本没有区别，但节省了大量用于购买IT产品、技术和维护运行的资金，且像打开自来水龙头就能用水一样，方便地利用信息化系统，从而大幅度降低了中小企业信息化的门槛与风险。 SaaS服务的优势 对企业来说，SaaS的优点在于： ⒈ 从技术方面来看：企业无需再配备IT方面的专业技术人员，同时又能得到最新的技术应用，满足企业对信息管理的需求。⒉ 从投资方面来看：企业只以相对低廉的“月费”方式投资，不用一次性投资到位，不占用过多的营运资金，从而缓解企业资金不足的压力；不用考虑成本折旧问题，并能及时获得最新硬件平台及最佳解决方案。⒊ 从维护和管理方面来看：由于企业采取租用的方式来进行物流业务管理，不需要专门的维护和管理人员，也不需要为维护和管理人员支付额外费用。很大程度上缓解企业在人力、财力上的压力，使其能够集中资金对核心业务进行有效的运营。 以上是对SOA、ERP、SAP和SaaS四者的分析。","categories":[],"tags":[]},{"title":"MySQL循环批量插入数据","slug":"mysql循环批量插入数据","date":"2010-05-31T06:37:03.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/f02121e8.html","link":"","permalink":"https://1630425.github.io/posts/f02121e8.html","excerpt":"","text":"123456789101112TRUNCATE TABLE a; #清空表数据DROP PROCEDURE IF EXISTS proc_init_data; #如果存在此存储过程则删掉DELIMITER $ -- 使用delimiter后，将不把分号当做语句结束，会将该段整个提交CREATE PROCEDURE proc_init_data()BEGIN DECLARE i INT DEFAULT 1; WHILE i&lt;=10000 DO INSERT INTO a (`name`) VALUES (i); SET i = i+1; END WHILE;END $CALL proc_init_data();","categories":[{"name":"数据库","slug":"数据库","permalink":"https://1630425.github.io/categories/数据库/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://1630425.github.io/tags/MySQL/"}]},{"title":"MySQL删除表数据DROP、TRUNCATE和DELETE的用法和区别","slug":"mysql删除表数据drop、truncate和delete的用法和区别","date":"2010-05-30T06:21:46.000Z","updated":"2019-06-05T03:03:41.052Z","comments":true,"path":"posts/fd4a1da4.html","link":"","permalink":"https://1630425.github.io/posts/fd4a1da4.html","excerpt":"","text":"程度从强到弱drop table tbdrop将表格直接删除，没有办法找回 truncate (table) tb删除表中的所有数据，不能与where一起使用 delete from tb (where)删除表中的数据(可制定某一行) 区别：truncate和delete的区别1、事务：truncate是不可以rollback的，但是delete是可以rollback的； 原因：truncate删除整表数据(ddl语句,隐式提交)，delete是一行一行的删除，可以rollback，效率上truncate比delete快，但truncate删除后不记录mysql日志，不可以恢复数据。 2、效果：truncate删除后将重新水平线和索引(id从零开始) ,delete不会删除索引，truncate相当于保留mysql表的结构，重新创建了这个表，所有的状态都相当于新表。 3、truncate 不能触发任何Delete触发器。 4、delete 删除可以返回行数","categories":[{"name":"数据库","slug":"数据库","permalink":"https://1630425.github.io/categories/数据库/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://1630425.github.io/tags/MySQL/"}]}]}